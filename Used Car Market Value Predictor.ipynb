{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rusty Bargain used car sales service is developing an app to attract new customers. In that app, you can quickly find out the market value of your car. You have access to historical data: technical specifications, trim versions, and prices. You need to build the model to determine the value. \n",
    "\n",
    "Rusty Bargain is interested in:\n",
    "\n",
    "- the quality of the prediction;\n",
    "- the speed of the prediction;\n",
    "- the time required for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>NotRepaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>24/03/2016 11:52</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>70435</td>\n",
       "      <td>07/04/2016 03:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24/03/2016 10:58</td>\n",
       "      <td>18300</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manual</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>audi</td>\n",
       "      <td>yes</td>\n",
       "      <td>24/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>66954</td>\n",
       "      <td>07/04/2016 01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14/03/2016 12:52</td>\n",
       "      <td>9800</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>auto</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>90480</td>\n",
       "      <td>05/04/2016 12:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17/03/2016 16:54</td>\n",
       "      <td>1500</td>\n",
       "      <td>small</td>\n",
       "      <td>2001</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>17/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91074</td>\n",
       "      <td>17/03/2016 17:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31/03/2016 17:25</td>\n",
       "      <td>3600</td>\n",
       "      <td>small</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>skoda</td>\n",
       "      <td>no</td>\n",
       "      <td>31/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>60437</td>\n",
       "      <td>06/04/2016 10:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DateCrawled  Price VehicleType  RegistrationYear Gearbox  Power  \\\n",
       "0  24/03/2016 11:52    480         NaN              1993  manual      0   \n",
       "1  24/03/2016 10:58  18300       coupe              2011  manual    190   \n",
       "2  14/03/2016 12:52   9800         suv              2004    auto    163   \n",
       "3  17/03/2016 16:54   1500       small              2001  manual     75   \n",
       "4  31/03/2016 17:25   3600       small              2008  manual     69   \n",
       "\n",
       "   Model  Mileage  RegistrationMonth  FuelType       Brand NotRepaired  \\\n",
       "0   golf   150000                  0    petrol  volkswagen         NaN   \n",
       "1    NaN   125000                  5  gasoline        audi         yes   \n",
       "2  grand   125000                  8  gasoline        jeep         NaN   \n",
       "3   golf   150000                  6    petrol  volkswagen          no   \n",
       "4  fabia    90000                  7  gasoline       skoda          no   \n",
       "\n",
       "        DateCreated  NumberOfPictures  PostalCode          LastSeen  \n",
       "0  24/03/2016 00:00                 0       70435  07/04/2016 03:16  \n",
       "1  24/03/2016 00:00                 0       66954  07/04/2016 01:46  \n",
       "2  14/03/2016 00:00                 0       90480  05/04/2016 12:47  \n",
       "3  17/03/2016 00:00                 0       91074  17/03/2016 17:40  \n",
       "4  31/03/2016 00:00                 0       60437  06/04/2016 10:17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354369 entries, 0 to 354368\n",
      "Data columns (total 16 columns):\n",
      "DateCrawled          354369 non-null object\n",
      "Price                354369 non-null int64\n",
      "VehicleType          316879 non-null object\n",
      "RegistrationYear     354369 non-null int64\n",
      "Gearbox              334536 non-null object\n",
      "Power                354369 non-null int64\n",
      "Model                334664 non-null object\n",
      "Mileage              354369 non-null int64\n",
      "RegistrationMonth    354369 non-null int64\n",
      "FuelType             321474 non-null object\n",
      "Brand                354369 non-null object\n",
      "NotRepaired          283215 non-null object\n",
      "DateCreated          354369 non-null object\n",
      "NumberOfPictures     354369 non-null int64\n",
      "PostalCode           354369 non-null int64\n",
      "LastSeen             354369 non-null object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 43.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.0</td>\n",
       "      <td>354369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4416.656776</td>\n",
       "      <td>2004.234448</td>\n",
       "      <td>110.094337</td>\n",
       "      <td>128211.172535</td>\n",
       "      <td>5.714645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50508.689087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>4514.158514</td>\n",
       "      <td>90.227958</td>\n",
       "      <td>189.850405</td>\n",
       "      <td>37905.341530</td>\n",
       "      <td>3.726421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25783.096248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6400.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99998.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Price  RegistrationYear          Power        Mileage  \\\n",
       "count  354369.000000     354369.000000  354369.000000  354369.000000   \n",
       "mean     4416.656776       2004.234448     110.094337  128211.172535   \n",
       "std      4514.158514         90.227958     189.850405   37905.341530   \n",
       "min         0.000000       1000.000000       0.000000    5000.000000   \n",
       "25%      1050.000000       1999.000000      69.000000  125000.000000   \n",
       "50%      2700.000000       2003.000000     105.000000  150000.000000   \n",
       "75%      6400.000000       2008.000000     143.000000  150000.000000   \n",
       "max     20000.000000       9999.000000   20000.000000  150000.000000   \n",
       "\n",
       "       RegistrationMonth  NumberOfPictures     PostalCode  \n",
       "count      354369.000000          354369.0  354369.000000  \n",
       "mean            5.714645               0.0   50508.689087  \n",
       "std             3.726421               0.0   25783.096248  \n",
       "min             0.000000               0.0    1067.000000  \n",
       "25%             3.000000               0.0   30165.000000  \n",
       "50%             6.000000               0.0   49413.000000  \n",
       "75%             9.000000               0.0   71083.000000  \n",
       "max            12.000000               0.0   99998.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NotRepaired'] = df['NotRepaired'].fillna('unknown')\n",
    "df['FuelType'] = df['FuelType'].fillna('unknown')\n",
    "df['VehicleType'] = df['VehicleType'].fillna('unknown')\n",
    "df['Model'] = df['Model'].fillna('unknown')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless = ['DateCrawled', 'DateCreated', 'PostalCode', 'LastSeen', 'NumberOfPictures']\n",
    "df.drop(useless, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than drop missing data, the best approach here is to fill in unknown values as \"unknown.\" The reason for this with the NotRepaired category should be apparent right away -without knowning if its repaired, there's no way to accurately fill missing values. And, since about 20% of the data points are null, cutting those out can cause an unreasonable loss of data. \n",
    "\n",
    "Understanding cars, it can also be difficult to infer model, fueltype, or vehicletype in any automated fashion. With that in mind, filling each of these with \"unknown\" could lead to better results than simply dropping the data. If I find later that the model is not performing as well as hoped, I may revisit this to drop nulls instead. But, losing that much data is not preferred. \n",
    "\n",
    "Finally, I proceeded to drop additional columns for features that were unecessary and separate into X and y variables (features/target) for proper model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "X2 = X\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit(X2)\n",
    "data_ordinal = encoder.transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 623 ms, total: 1.73 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LinearRegression()\n",
    "score = cross_val_score(model, data_ordinal, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Score: 0.5148508013412083\n"
     ]
    }
   ],
   "source": [
    "print('Average Model Score:', pd.np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_ordinal, y, test_size=0.25, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 ms, sys: 15.8 ms, total: 116 ms\n",
      "Wall time: 136 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 ms, sys: 33.5 ms, total: 62 ms\n",
      "Wall time: 88.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3170.8779260043275"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 640 ms, total: 1min 4s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = RandomForestRegressor()\n",
    "score = cross_val_score(model, data_ordinal, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Score: 0.8464434755619253\n"
     ]
    }
   ],
   "source": [
    "print('Average Model Score:', pd.np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 156 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1812.5574508909638\n",
      "CPU times: user 480 ms, sys: 83 µs, total: 480 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict = model.predict(X_test)\n",
    "rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 50 : 1758.507408287913 cross_val_score: 0.8556229260274224\n",
      "n_estimators = 60 : 1756.8463763566556 cross_val_score: 0.8561371063720357\n",
      "n_estimators = 70 : 1753.701416345681 cross_val_score: 0.8565141115808028\n",
      "n_estimators = 80 : 1752.254240099608 cross_val_score: 0.8567392983275539\n",
      "n_estimators = 90 : 1751.210969959759 cross_val_score: 0.8568680145109789\n",
      "n_estimators = 100 : 1751.1072075470656 cross_val_score: 0.8569945307911786\n",
      "CPU times: user 55min 57s, sys: 30.3 s, total: 56min 28s\n",
      "Wall time: 56min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for estims in range(50, 101, 10):\n",
    "    model = RandomForestRegressor(random_state=47, n_estimators=estims)\n",
    "    score = cross_val_score(model, data_ordinal, y, cv=5, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "    print(\"n_estimators =\", estims, \":\", rmse, \"cross_val_score:\", score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Saving some server time:</strong>\n",
    "To save some processessing time, I cut the loops I was testing in half this time. Last time I did range(10,101,10) but since I'm adding CV as instructed, I did this to cut back processing time. \n",
    "\n",
    "I'll cut all future loops as well to make this uniform and report on time spent accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth= 10 : 2021.8464652764878 cross_val_score: 0.8062097161017145\n",
      "max_depth= 20 : 1735.421750860292 cross_val_score: 0.8594758134244804\n",
      "max_depth= 30 : 1749.6142593486973 cross_val_score: 0.8570749135665074\n",
      "max_depth= 40 : 1750.8121739120375 cross_val_score: 0.8569580713596269\n",
      "max_depth= 50 : 1751.1072075470656 cross_val_score: 0.8569945291444683\n",
      "CPU times: user 52min 9s, sys: 16.5 s, total: 52min 25s\n",
      "Wall time: 52min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for depth in range(10, 51, 10):\n",
    "    model = RandomForestRegressor(random_state=47, n_estimators=100, max_depth=depth)\n",
    "    score = cross_val_score(model, data_ordinal, y, cv=5, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "    print(\"max_depth=\", depth, \":\", rmse, \"cross_val_score:\", score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Score: 0.8570749135665074\n",
      "CPU times: user 10min 3s, sys: 2.54 s, total: 10min 6s\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = RandomForestRegressor(random_state=47, n_estimators=100, max_depth=30)\n",
    "score = cross_val_score(model, data_ordinal, y, cv=5)\n",
    "print('Average Model Score:', pd.np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 452 ms, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                      n_jobs=None, oob_score=False, random_state=47, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1749.6142593486973\n",
      "CPU times: user 4.77 s, sys: 70 µs, total: 4.77 s\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict = model.predict(X_test)\n",
    "rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Random Forest has proven better than Linear Regression, the RMSE is still high. Even with some hyperparamter tuning, we definitely want to look at other models to achieve better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "categorical = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'NotRepaired']\n",
    "for col in categorical:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "    X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'num_leaves': range(100, 201, 10),\n",
    "        'max_depth': range(10, 51, 10)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] num_leaves=110, max_depth=50 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=110, max_depth=50, score=0.863, total=  17.1s\n",
      "[CV] num_leaves=110, max_depth=50 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=110, max_depth=50, score=0.862, total=  17.2s\n",
      "[CV] num_leaves=110, max_depth=50 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   34.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=110, max_depth=50, score=0.864, total=  38.6s\n",
      "[CV] num_leaves=170, max_depth=40 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=170, max_depth=40, score=0.865, total=  19.8s\n",
      "[CV] num_leaves=170, max_depth=40 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=170, max_depth=40, score=0.865, total=  19.8s\n",
      "[CV] num_leaves=170, max_depth=40 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=170, max_depth=40, score=0.866, total=  21.3s\n",
      "[CV] num_leaves=200, max_depth=30 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=200, max_depth=30, score=0.866, total=  21.5s\n",
      "[CV] num_leaves=200, max_depth=30 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  2.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=200, max_depth=30, score=0.866, total=  22.0s\n",
      "[CV] num_leaves=200, max_depth=30 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=200, max_depth=30, score=0.867, total=  21.7s\n",
      "[CV] num_leaves=120, max_depth=40 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ num_leaves=120, max_depth=40, score=0.863, total=  18.7s\n",
      "[CV] num_leaves=120, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=40, score=0.863, total=  16.8s\n",
      "[CV] num_leaves=120, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=40, score=0.864, total=  18.8s\n",
      "[CV] num_leaves=130, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=50, score=0.864, total=  18.6s\n",
      "[CV] num_leaves=130, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=50, score=0.864, total=  18.7s\n",
      "[CV] num_leaves=130, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=50, score=0.865, total=  17.8s\n",
      "[CV] num_leaves=180, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=40, score=0.865, total=  20.9s\n",
      "[CV] num_leaves=180, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=40, score=0.866, total=  22.2s\n",
      "[CV] num_leaves=180, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=40, score=0.867, total=  20.4s\n",
      "[CV] num_leaves=130, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=40, score=0.864, total=  17.4s\n",
      "[CV] num_leaves=130, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=40, score=0.864, total=  17.5s\n",
      "[CV] num_leaves=130, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=40, score=0.865, total=  18.2s\n",
      "[CV] num_leaves=120, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=30, score=0.863, total=  17.1s\n",
      "[CV] num_leaves=120, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=30, score=0.863, total=  18.4s\n",
      "[CV] num_leaves=120, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=30, score=0.864, total=  17.1s\n",
      "[CV] num_leaves=110, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=110, max_depth=20, score=0.863, total=  15.9s\n",
      "[CV] num_leaves=110, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=110, max_depth=20, score=0.862, total=  16.0s\n",
      "[CV] num_leaves=110, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=110, max_depth=20, score=0.864, total=  16.0s\n",
      "[CV] num_leaves=130, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=20, score=0.864, total=  17.6s\n",
      "[CV] num_leaves=130, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=20, score=0.864, total=  18.9s\n",
      "[CV] num_leaves=130, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=130, max_depth=20, score=0.865, total=  18.8s\n",
      "[CV] num_leaves=100, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=50, score=0.862, total=  16.1s\n",
      "[CV] num_leaves=100, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=50, score=0.861, total=  15.1s\n",
      "[CV] num_leaves=100, max_depth=50 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=50, score=0.863, total=  18.8s\n",
      "[CV] num_leaves=140, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=140, max_depth=40, score=0.864, total=  17.3s\n",
      "[CV] num_leaves=140, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=140, max_depth=40, score=0.864, total=  18.7s\n",
      "[CV] num_leaves=140, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=140, max_depth=40, score=0.865, total=  18.0s\n",
      "[CV] num_leaves=180, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=30, score=0.865, total=  26.0s\n",
      "[CV] num_leaves=180, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=30, score=0.866, total=  21.3s\n",
      "[CV] num_leaves=180, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=180, max_depth=30, score=0.867, total=  20.6s\n",
      "[CV] num_leaves=100, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=40, score=0.862, total=  17.7s\n",
      "[CV] num_leaves=100, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=40, score=0.861, total= 1.8min\n",
      "[CV] num_leaves=100, max_depth=40 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=40, score=0.863, total=  20.3s\n",
      "[CV] num_leaves=120, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=20, score=0.863, total=  17.5s\n",
      "[CV] num_leaves=120, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=20, score=0.863, total=  19.0s\n",
      "[CV] num_leaves=120, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=120, max_depth=20, score=0.864, total=  16.6s\n",
      "[CV] num_leaves=150, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=150, max_depth=30, score=0.864, total=  18.9s\n",
      "[CV] num_leaves=150, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=150, max_depth=30, score=0.864, total=  19.1s\n",
      "[CV] num_leaves=150, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=150, max_depth=30, score=0.866, total=  18.9s\n",
      "[CV] num_leaves=170, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=30, score=0.865, total=  20.5s\n",
      "[CV] num_leaves=170, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=30, score=0.865, total=  19.8s\n",
      "[CV] num_leaves=170, max_depth=30 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=30, score=0.866, total=  20.9s\n",
      "[CV] num_leaves=170, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=20, score=0.865, total=  20.1s\n",
      "[CV] num_leaves=170, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=20, score=0.865, total=  19.9s\n",
      "[CV] num_leaves=170, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=170, max_depth=20, score=0.866, total=  19.7s\n",
      "[CV] num_leaves=200, max_depth=10 ....................................\n",
      "[CV] ........ num_leaves=200, max_depth=10, score=0.865, total=  20.6s\n",
      "[CV] num_leaves=200, max_depth=10 ....................................\n",
      "[CV] ........ num_leaves=200, max_depth=10, score=0.864, total=  19.7s\n",
      "[CV] num_leaves=200, max_depth=10 ....................................\n",
      "[CV] ........ num_leaves=200, max_depth=10, score=0.865, total=  20.8s\n",
      "[CV] num_leaves=100, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=20, score=0.862, total=  15.8s\n",
      "[CV] num_leaves=100, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=20, score=0.862, total=  20.1s\n",
      "[CV] num_leaves=100, max_depth=20 ....................................\n",
      "[CV] ........ num_leaves=100, max_depth=20, score=0.863, total=  16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 20.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 47s, sys: 7.36 s, total: 20min 54s\n",
      "Wall time: 21min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=LGBMRegressor(boosting_type='gbdt',\n",
       "                                           class_weight=None,\n",
       "                                           colsample_bytree=1.0,\n",
       "                                           importance_type='split',\n",
       "                                           learning_rate=0.1, max_depth=-1,\n",
       "                                           min_child_samples=20,\n",
       "                                           min_child_weight=0.001,\n",
       "                                           min_split_gain=0.0, n_estimators=100,\n",
       "                                           n_jobs=-1, num_leaves=31,\n",
       "                                           objective=None, random_state=None,\n",
       "                                           reg_alpha=0.0, reg_lambda=0.0,\n",
       "                                           silent=True, subsample=1.0,\n",
       "                                           subsample_for_bin=200000,\n",
       "                                           subsample_freq=0),\n",
       "                   iid='warn', n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'max_depth': range(10, 51, 10),\n",
       "                                        'num_leaves': range(100, 201, 10)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=47, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "lgb = LGBMRegressor()\n",
    "lgbm_random = RandomizedSearchCV(estimator = lgb, param_distributions = params, \n",
    "                         n_iter = 20, cv = 3, verbose=10, random_state=47)\n",
    "lgbm_random.fit(X, y, categorical_feature='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 120 ms, total: 20 s\n",
      "Wall time: 20.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=30,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=180, objective=None,\n",
       "              random_state=47, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgb = LGBMRegressor(random_state=47, num_leaves=180, max_depth=30)\n",
    "lgb.fit(X_train, y_train, categorical_feature='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 7.77 ms, total: 2.56 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict = lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1694.4045196247666\n"
     ]
    }
   ],
   "source": [
    "print('RMSE:', pd.np.sqrt(mean_squared_error(y_test, predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the base LGBM Regressor does not do any better than a Random Forest. With that in mind, we'll want to try some hpyerparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tuning both the number of leaves and max depth of the LGBM Regressor, we can improve performance, but not by much. With some more time, we could look at tuning additional parameters. Instead, I would prefer to explore CatBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4464.4408376\ttotal: 938ms\tremaining: 15m 37s\n",
      "1:\tlearn: 4379.6713831\ttotal: 1.84s\tremaining: 15m 17s\n",
      "2:\tlearn: 4298.4704952\ttotal: 2.82s\tremaining: 15m 38s\n",
      "3:\tlearn: 4219.5489163\ttotal: 3.53s\tremaining: 14m 38s\n",
      "4:\tlearn: 4143.6048682\ttotal: 4.23s\tremaining: 14m 1s\n",
      "5:\tlearn: 4070.6965703\ttotal: 5.03s\tremaining: 13m 53s\n",
      "6:\tlearn: 4001.1528382\ttotal: 5.82s\tremaining: 13m 46s\n",
      "7:\tlearn: 3934.4175398\ttotal: 6.53s\tremaining: 13m 29s\n",
      "8:\tlearn: 3870.3856141\ttotal: 7.32s\tremaining: 13m 26s\n",
      "9:\tlearn: 3809.9134385\ttotal: 8.12s\tremaining: 13m 24s\n",
      "10:\tlearn: 3749.7410635\ttotal: 8.93s\tremaining: 13m 22s\n",
      "11:\tlearn: 3692.7706487\ttotal: 9.63s\tremaining: 13m 12s\n",
      "12:\tlearn: 3635.0990493\ttotal: 10.4s\tremaining: 13m 11s\n",
      "13:\tlearn: 3581.1875857\ttotal: 11.2s\tremaining: 13m 10s\n",
      "14:\tlearn: 3529.5560317\ttotal: 12s\tremaining: 13m 8s\n",
      "15:\tlearn: 3478.1719445\ttotal: 12.7s\tremaining: 13m 2s\n",
      "16:\tlearn: 3430.2633445\ttotal: 13.5s\tremaining: 13m 2s\n",
      "17:\tlearn: 3382.7286120\ttotal: 14.4s\tremaining: 13m 6s\n",
      "18:\tlearn: 3338.2801251\ttotal: 15.1s\tremaining: 13m\n",
      "19:\tlearn: 3295.1626568\ttotal: 15.9s\tremaining: 12m 59s\n",
      "20:\tlearn: 3252.5499291\ttotal: 16.7s\tremaining: 12m 58s\n",
      "21:\tlearn: 3210.6779932\ttotal: 17.5s\tremaining: 12m 58s\n",
      "22:\tlearn: 3172.4451901\ttotal: 18.1s\tremaining: 12m 49s\n",
      "23:\tlearn: 3131.4950354\ttotal: 18.9s\tremaining: 12m 49s\n",
      "24:\tlearn: 3095.3353921\ttotal: 19.7s\tremaining: 12m 48s\n",
      "25:\tlearn: 3060.9993515\ttotal: 20.5s\tremaining: 12m 47s\n",
      "26:\tlearn: 3024.5016535\ttotal: 21.3s\tremaining: 12m 47s\n",
      "27:\tlearn: 2992.1639692\ttotal: 21.9s\tremaining: 12m 40s\n",
      "28:\tlearn: 2958.6018634\ttotal: 22.8s\tremaining: 12m 43s\n",
      "29:\tlearn: 2925.7425354\ttotal: 23.5s\tremaining: 12m 40s\n",
      "30:\tlearn: 2895.0243008\ttotal: 24.3s\tremaining: 12m 39s\n",
      "31:\tlearn: 2867.6360543\ttotal: 25.1s\tremaining: 12m 39s\n",
      "32:\tlearn: 2838.5898700\ttotal: 25.9s\tremaining: 12m 38s\n",
      "33:\tlearn: 2812.3137066\ttotal: 26.7s\tremaining: 12m 38s\n",
      "34:\tlearn: 2786.6177040\ttotal: 27.6s\tremaining: 12m 41s\n",
      "35:\tlearn: 2762.4017560\ttotal: 28.5s\tremaining: 12m 42s\n",
      "36:\tlearn: 2738.6798821\ttotal: 29.2s\tremaining: 12m 39s\n",
      "37:\tlearn: 2716.6917599\ttotal: 29.9s\tremaining: 12m 36s\n",
      "38:\tlearn: 2693.0822617\ttotal: 30.7s\tremaining: 12m 36s\n",
      "39:\tlearn: 2671.1183770\ttotal: 31.5s\tremaining: 12m 35s\n",
      "40:\tlearn: 2651.4343571\ttotal: 32.3s\tremaining: 12m 35s\n",
      "41:\tlearn: 2630.3513801\ttotal: 32.9s\tremaining: 12m 30s\n",
      "42:\tlearn: 2611.5192922\ttotal: 33.7s\tremaining: 12m 29s\n",
      "43:\tlearn: 2593.7377136\ttotal: 34.5s\tremaining: 12m 29s\n",
      "44:\tlearn: 2576.3127541\ttotal: 35.2s\tremaining: 12m 26s\n",
      "45:\tlearn: 2555.6959715\ttotal: 36s\tremaining: 12m 26s\n",
      "46:\tlearn: 2537.7675041\ttotal: 36.9s\tremaining: 12m 27s\n",
      "47:\tlearn: 2520.2221290\ttotal: 37.6s\tremaining: 12m 25s\n",
      "48:\tlearn: 2502.3623675\ttotal: 38.4s\tremaining: 12m 25s\n",
      "49:\tlearn: 2486.1681895\ttotal: 39.1s\tremaining: 12m 22s\n",
      "50:\tlearn: 2470.7767530\ttotal: 39.8s\tremaining: 12m 20s\n",
      "51:\tlearn: 2453.9789874\ttotal: 40.4s\tremaining: 12m 16s\n",
      "52:\tlearn: 2439.7329445\ttotal: 41s\tremaining: 12m 12s\n",
      "53:\tlearn: 2425.5635164\ttotal: 41.8s\tremaining: 12m 11s\n",
      "54:\tlearn: 2411.1278653\ttotal: 42.6s\tremaining: 12m 11s\n",
      "55:\tlearn: 2398.2752743\ttotal: 43.3s\tremaining: 12m 9s\n",
      "56:\tlearn: 2384.6540126\ttotal: 44.1s\tremaining: 12m 9s\n",
      "57:\tlearn: 2371.8895510\ttotal: 44.7s\tremaining: 12m 5s\n",
      "58:\tlearn: 2360.2805595\ttotal: 45.5s\tremaining: 12m 5s\n",
      "59:\tlearn: 2347.9289835\ttotal: 46.3s\tremaining: 12m 4s\n",
      "60:\tlearn: 2336.8392583\ttotal: 47.2s\tremaining: 12m 5s\n",
      "61:\tlearn: 2326.5032507\ttotal: 48.1s\tremaining: 12m 7s\n",
      "62:\tlearn: 2315.8024428\ttotal: 48.8s\tremaining: 12m 5s\n",
      "63:\tlearn: 2306.0431978\ttotal: 49.5s\tremaining: 12m 3s\n",
      "64:\tlearn: 2295.7914944\ttotal: 50.2s\tremaining: 12m 1s\n",
      "65:\tlearn: 2286.7588976\ttotal: 50.9s\tremaining: 11m 59s\n",
      "66:\tlearn: 2278.1221659\ttotal: 51.5s\tremaining: 11m 56s\n",
      "67:\tlearn: 2269.1033274\ttotal: 52.3s\tremaining: 11m 56s\n",
      "68:\tlearn: 2260.3960251\ttotal: 53s\tremaining: 11m 54s\n",
      "69:\tlearn: 2252.4051376\ttotal: 53.6s\tremaining: 11m 52s\n",
      "70:\tlearn: 2244.6032161\ttotal: 54.5s\tremaining: 11m 52s\n",
      "71:\tlearn: 2236.7941131\ttotal: 55.2s\tremaining: 11m 52s\n",
      "72:\tlearn: 2228.9590567\ttotal: 56s\tremaining: 11m 51s\n",
      "73:\tlearn: 2221.5939217\ttotal: 56.6s\tremaining: 11m 48s\n",
      "74:\tlearn: 2214.7503066\ttotal: 57.4s\tremaining: 11m 47s\n",
      "75:\tlearn: 2207.6772372\ttotal: 58.1s\tremaining: 11m 46s\n",
      "76:\tlearn: 2201.1923463\ttotal: 58.7s\tremaining: 11m 44s\n",
      "77:\tlearn: 2195.4952977\ttotal: 59.5s\tremaining: 11m 43s\n",
      "78:\tlearn: 2189.3739092\ttotal: 1m\tremaining: 11m 43s\n",
      "79:\tlearn: 2183.1155021\ttotal: 1m 1s\tremaining: 11m 43s\n",
      "80:\tlearn: 2177.0919660\ttotal: 1m 1s\tremaining: 11m 41s\n",
      "81:\tlearn: 2171.9434448\ttotal: 1m 2s\tremaining: 11m 40s\n",
      "82:\tlearn: 2166.5470561\ttotal: 1m 3s\tremaining: 11m 38s\n",
      "83:\tlearn: 2160.9347508\ttotal: 1m 4s\tremaining: 11m 39s\n",
      "84:\tlearn: 2155.6021025\ttotal: 1m 4s\tremaining: 11m 39s\n",
      "85:\tlearn: 2150.6681214\ttotal: 1m 5s\tremaining: 11m 37s\n",
      "86:\tlearn: 2145.9264618\ttotal: 1m 6s\tremaining: 11m 36s\n",
      "87:\tlearn: 2141.5533944\ttotal: 1m 6s\tremaining: 11m 33s\n",
      "88:\tlearn: 2136.5355984\ttotal: 1m 7s\tremaining: 11m 34s\n",
      "89:\tlearn: 2132.1350738\ttotal: 1m 8s\tremaining: 11m 33s\n",
      "90:\tlearn: 2127.6137857\ttotal: 1m 9s\tremaining: 11m 32s\n",
      "91:\tlearn: 2123.2311341\ttotal: 1m 10s\tremaining: 11m 31s\n",
      "92:\tlearn: 2119.4538561\ttotal: 1m 10s\tremaining: 11m 29s\n",
      "93:\tlearn: 2115.4915083\ttotal: 1m 11s\tremaining: 11m 28s\n",
      "94:\tlearn: 2111.9860672\ttotal: 1m 12s\tremaining: 11m 28s\n",
      "95:\tlearn: 2108.2778839\ttotal: 1m 12s\tremaining: 11m 25s\n",
      "96:\tlearn: 2104.9003582\ttotal: 1m 13s\tremaining: 11m 25s\n",
      "97:\tlearn: 2100.5244931\ttotal: 1m 14s\tremaining: 11m 24s\n",
      "98:\tlearn: 2097.1373809\ttotal: 1m 15s\tremaining: 11m 23s\n",
      "99:\tlearn: 2093.4479379\ttotal: 1m 15s\tremaining: 11m 23s\n",
      "100:\tlearn: 2089.9515823\ttotal: 1m 16s\tremaining: 11m 22s\n",
      "101:\tlearn: 2086.5171824\ttotal: 1m 17s\tremaining: 11m 22s\n",
      "102:\tlearn: 2083.0914479\ttotal: 1m 18s\tremaining: 11m 22s\n",
      "103:\tlearn: 2080.0944588\ttotal: 1m 19s\tremaining: 11m 22s\n",
      "104:\tlearn: 2077.3494793\ttotal: 1m 20s\tremaining: 11m 22s\n",
      "105:\tlearn: 2074.3053320\ttotal: 1m 20s\tremaining: 11m 20s\n",
      "106:\tlearn: 2071.4247087\ttotal: 1m 21s\tremaining: 11m 21s\n",
      "107:\tlearn: 2068.6670297\ttotal: 1m 22s\tremaining: 11m 19s\n",
      "108:\tlearn: 2066.1377041\ttotal: 1m 23s\tremaining: 11m 18s\n",
      "109:\tlearn: 2063.7274428\ttotal: 1m 23s\tremaining: 11m 17s\n",
      "110:\tlearn: 2061.2885988\ttotal: 1m 24s\tremaining: 11m 15s\n",
      "111:\tlearn: 2058.2530642\ttotal: 1m 25s\tremaining: 11m 14s\n",
      "112:\tlearn: 2055.9754774\ttotal: 1m 25s\tremaining: 11m 14s\n",
      "113:\tlearn: 2053.0047559\ttotal: 1m 26s\tremaining: 11m 13s\n",
      "114:\tlearn: 2050.4546770\ttotal: 1m 27s\tremaining: 11m 11s\n",
      "115:\tlearn: 2048.3367821\ttotal: 1m 28s\tremaining: 11m 11s\n",
      "116:\tlearn: 2046.4989686\ttotal: 1m 28s\tremaining: 11m 10s\n",
      "117:\tlearn: 2043.8817917\ttotal: 1m 29s\tremaining: 11m 8s\n",
      "118:\tlearn: 2041.8501184\ttotal: 1m 30s\tremaining: 11m 7s\n",
      "119:\tlearn: 2039.3924763\ttotal: 1m 30s\tremaining: 11m 7s\n",
      "120:\tlearn: 2037.0643874\ttotal: 1m 31s\tremaining: 11m 5s\n",
      "121:\tlearn: 2034.4177680\ttotal: 1m 32s\tremaining: 11m 4s\n",
      "122:\tlearn: 2032.4918632\ttotal: 1m 33s\tremaining: 11m 3s\n",
      "123:\tlearn: 2030.0705553\ttotal: 1m 33s\tremaining: 11m 3s\n",
      "124:\tlearn: 2028.2066028\ttotal: 1m 34s\tremaining: 11m 3s\n",
      "125:\tlearn: 2026.2594531\ttotal: 1m 35s\tremaining: 11m 3s\n",
      "126:\tlearn: 2024.2418995\ttotal: 1m 36s\tremaining: 11m 2s\n",
      "127:\tlearn: 2022.4722889\ttotal: 1m 37s\tremaining: 11m 2s\n",
      "128:\tlearn: 2020.4420261\ttotal: 1m 37s\tremaining: 11m 1s\n",
      "129:\tlearn: 2018.4079302\ttotal: 1m 38s\tremaining: 11m\n",
      "130:\tlearn: 2016.8155758\ttotal: 1m 39s\tremaining: 11m\n",
      "131:\tlearn: 2015.0631100\ttotal: 1m 40s\tremaining: 10m 59s\n",
      "132:\tlearn: 2013.2827707\ttotal: 1m 41s\tremaining: 10m 59s\n",
      "133:\tlearn: 2011.6137495\ttotal: 1m 41s\tremaining: 10m 58s\n",
      "134:\tlearn: 2010.0344600\ttotal: 1m 42s\tremaining: 10m 56s\n",
      "135:\tlearn: 2008.3212633\ttotal: 1m 43s\tremaining: 10m 56s\n",
      "136:\tlearn: 2006.7936829\ttotal: 1m 44s\tremaining: 10m 55s\n",
      "137:\tlearn: 2004.9513559\ttotal: 1m 44s\tremaining: 10m 55s\n",
      "138:\tlearn: 2003.6798174\ttotal: 1m 45s\tremaining: 10m 53s\n",
      "139:\tlearn: 2002.1866024\ttotal: 1m 46s\tremaining: 10m 52s\n",
      "140:\tlearn: 2000.7263117\ttotal: 1m 46s\tremaining: 10m 51s\n",
      "141:\tlearn: 1999.3507691\ttotal: 1m 47s\tremaining: 10m 50s\n",
      "142:\tlearn: 1997.8007779\ttotal: 1m 48s\tremaining: 10m 50s\n",
      "143:\tlearn: 1996.0262619\ttotal: 1m 49s\tremaining: 10m 48s\n",
      "144:\tlearn: 1994.6702667\ttotal: 1m 50s\tremaining: 10m 49s\n",
      "145:\tlearn: 1993.5504325\ttotal: 1m 50s\tremaining: 10m 48s\n",
      "146:\tlearn: 1992.4172628\ttotal: 1m 51s\tremaining: 10m 46s\n",
      "147:\tlearn: 1991.2056153\ttotal: 1m 52s\tremaining: 10m 45s\n",
      "148:\tlearn: 1989.7951339\ttotal: 1m 52s\tremaining: 10m 45s\n",
      "149:\tlearn: 1988.5822901\ttotal: 1m 53s\tremaining: 10m 44s\n",
      "150:\tlearn: 1987.4811318\ttotal: 1m 54s\tremaining: 10m 44s\n",
      "151:\tlearn: 1986.2609403\ttotal: 1m 55s\tremaining: 10m 43s\n",
      "152:\tlearn: 1985.0753506\ttotal: 1m 56s\tremaining: 10m 43s\n",
      "153:\tlearn: 1983.6580030\ttotal: 1m 56s\tremaining: 10m 42s\n",
      "154:\tlearn: 1982.5099018\ttotal: 1m 57s\tremaining: 10m 42s\n",
      "155:\tlearn: 1981.3486592\ttotal: 1m 58s\tremaining: 10m 42s\n",
      "156:\tlearn: 1980.2559579\ttotal: 1m 59s\tremaining: 10m 41s\n",
      "157:\tlearn: 1979.1590857\ttotal: 2m\tremaining: 10m 40s\n",
      "158:\tlearn: 1978.1278185\ttotal: 2m\tremaining: 10m 39s\n",
      "159:\tlearn: 1977.0041043\ttotal: 2m 1s\tremaining: 10m 39s\n",
      "160:\tlearn: 1975.8314301\ttotal: 2m 2s\tremaining: 10m 38s\n",
      "161:\tlearn: 1974.9684509\ttotal: 2m 3s\tremaining: 10m 37s\n",
      "162:\tlearn: 1973.7701721\ttotal: 2m 4s\tremaining: 10m 36s\n",
      "163:\tlearn: 1972.6230183\ttotal: 2m 4s\tremaining: 10m 36s\n",
      "164:\tlearn: 1971.5871897\ttotal: 2m 5s\tremaining: 10m 35s\n",
      "165:\tlearn: 1970.2829996\ttotal: 2m 6s\tremaining: 10m 34s\n",
      "166:\tlearn: 1969.5119027\ttotal: 2m 7s\tremaining: 10m 34s\n",
      "167:\tlearn: 1968.4955627\ttotal: 2m 8s\tremaining: 10m 34s\n",
      "168:\tlearn: 1967.4448446\ttotal: 2m 8s\tremaining: 10m 33s\n",
      "169:\tlearn: 1966.5528392\ttotal: 2m 9s\tremaining: 10m 32s\n",
      "170:\tlearn: 1965.8089639\ttotal: 2m 10s\tremaining: 10m 31s\n",
      "171:\tlearn: 1964.9155381\ttotal: 2m 11s\tremaining: 10m 30s\n",
      "172:\tlearn: 1964.0788252\ttotal: 2m 11s\tremaining: 10m 30s\n",
      "173:\tlearn: 1963.3019066\ttotal: 2m 12s\tremaining: 10m 29s\n",
      "174:\tlearn: 1962.7200990\ttotal: 2m 13s\tremaining: 10m 28s\n",
      "175:\tlearn: 1961.2553964\ttotal: 2m 13s\tremaining: 10m 27s\n",
      "176:\tlearn: 1960.3959635\ttotal: 2m 14s\tremaining: 10m 26s\n",
      "177:\tlearn: 1959.4199634\ttotal: 2m 15s\tremaining: 10m 24s\n",
      "178:\tlearn: 1958.7276891\ttotal: 2m 15s\tremaining: 10m 23s\n",
      "179:\tlearn: 1957.4065167\ttotal: 2m 16s\tremaining: 10m 23s\n",
      "180:\tlearn: 1956.6754873\ttotal: 2m 17s\tremaining: 10m 23s\n",
      "181:\tlearn: 1955.6170809\ttotal: 2m 18s\tremaining: 10m 23s\n",
      "182:\tlearn: 1954.8326377\ttotal: 2m 19s\tremaining: 10m 22s\n",
      "183:\tlearn: 1953.9684410\ttotal: 2m 20s\tremaining: 10m 21s\n",
      "184:\tlearn: 1953.2060584\ttotal: 2m 21s\tremaining: 10m 21s\n",
      "185:\tlearn: 1952.4900337\ttotal: 2m 21s\tremaining: 10m 20s\n",
      "186:\tlearn: 1951.8286078\ttotal: 2m 22s\tremaining: 10m 19s\n",
      "187:\tlearn: 1950.8614913\ttotal: 2m 23s\tremaining: 10m 19s\n",
      "188:\tlearn: 1950.0936177\ttotal: 2m 23s\tremaining: 10m 17s\n",
      "189:\tlearn: 1949.4321562\ttotal: 2m 24s\tremaining: 10m 17s\n",
      "190:\tlearn: 1948.8496317\ttotal: 2m 25s\tremaining: 10m 16s\n",
      "191:\tlearn: 1947.9861662\ttotal: 2m 26s\tremaining: 10m 15s\n",
      "192:\tlearn: 1946.9733755\ttotal: 2m 27s\tremaining: 10m 15s\n",
      "193:\tlearn: 1946.2138779\ttotal: 2m 27s\tremaining: 10m 14s\n",
      "194:\tlearn: 1945.3108996\ttotal: 2m 28s\tremaining: 10m 12s\n",
      "195:\tlearn: 1944.6042969\ttotal: 2m 29s\tremaining: 10m 12s\n",
      "196:\tlearn: 1943.5540536\ttotal: 2m 29s\tremaining: 10m 11s\n",
      "197:\tlearn: 1942.4057216\ttotal: 2m 30s\tremaining: 10m 10s\n",
      "198:\tlearn: 1941.8820269\ttotal: 2m 31s\tremaining: 10m 9s\n",
      "199:\tlearn: 1940.9706620\ttotal: 2m 32s\tremaining: 10m 8s\n",
      "200:\tlearn: 1940.4816978\ttotal: 2m 32s\tremaining: 10m 7s\n",
      "201:\tlearn: 1939.6997555\ttotal: 2m 33s\tremaining: 10m 6s\n",
      "202:\tlearn: 1939.1251093\ttotal: 2m 34s\tremaining: 10m 5s\n",
      "203:\tlearn: 1938.7050761\ttotal: 2m 35s\tremaining: 10m 5s\n",
      "204:\tlearn: 1938.1832330\ttotal: 2m 35s\tremaining: 10m 4s\n",
      "205:\tlearn: 1937.3703655\ttotal: 2m 36s\tremaining: 10m 3s\n",
      "206:\tlearn: 1936.6565582\ttotal: 2m 37s\tremaining: 10m 2s\n",
      "207:\tlearn: 1936.0562260\ttotal: 2m 38s\tremaining: 10m 2s\n",
      "208:\tlearn: 1935.2439913\ttotal: 2m 38s\tremaining: 10m 1s\n",
      "209:\tlearn: 1934.3094443\ttotal: 2m 39s\tremaining: 10m\n",
      "210:\tlearn: 1933.8363913\ttotal: 2m 40s\tremaining: 9m 59s\n",
      "211:\tlearn: 1933.1929733\ttotal: 2m 41s\tremaining: 9m 58s\n",
      "212:\tlearn: 1932.3578416\ttotal: 2m 41s\tremaining: 9m 57s\n",
      "213:\tlearn: 1931.8582084\ttotal: 2m 42s\tremaining: 9m 56s\n",
      "214:\tlearn: 1931.2542740\ttotal: 2m 43s\tremaining: 9m 55s\n",
      "215:\tlearn: 1930.4409537\ttotal: 2m 44s\tremaining: 9m 55s\n",
      "216:\tlearn: 1929.9847828\ttotal: 2m 44s\tremaining: 9m 54s\n",
      "217:\tlearn: 1929.2336623\ttotal: 2m 45s\tremaining: 9m 53s\n",
      "218:\tlearn: 1928.7458063\ttotal: 2m 46s\tremaining: 9m 52s\n",
      "219:\tlearn: 1928.1020073\ttotal: 2m 46s\tremaining: 9m 51s\n",
      "220:\tlearn: 1927.4882074\ttotal: 2m 47s\tremaining: 9m 51s\n",
      "221:\tlearn: 1926.9973596\ttotal: 2m 48s\tremaining: 9m 50s\n",
      "222:\tlearn: 1926.4804659\ttotal: 2m 49s\tremaining: 9m 49s\n",
      "223:\tlearn: 1925.4619861\ttotal: 2m 50s\tremaining: 9m 49s\n",
      "224:\tlearn: 1924.6261101\ttotal: 2m 50s\tremaining: 9m 48s\n",
      "225:\tlearn: 1923.9495674\ttotal: 2m 51s\tremaining: 9m 47s\n",
      "226:\tlearn: 1922.8214774\ttotal: 2m 52s\tremaining: 9m 47s\n",
      "227:\tlearn: 1922.1942930\ttotal: 2m 52s\tremaining: 9m 45s\n",
      "228:\tlearn: 1921.7542673\ttotal: 2m 53s\tremaining: 9m 44s\n",
      "229:\tlearn: 1921.1917317\ttotal: 2m 54s\tremaining: 9m 44s\n",
      "230:\tlearn: 1920.5714630\ttotal: 2m 55s\tremaining: 9m 43s\n",
      "231:\tlearn: 1919.7889567\ttotal: 2m 55s\tremaining: 9m 41s\n",
      "232:\tlearn: 1919.1113694\ttotal: 2m 56s\tremaining: 9m 40s\n",
      "233:\tlearn: 1918.7356077\ttotal: 2m 57s\tremaining: 9m 40s\n",
      "234:\tlearn: 1918.0469618\ttotal: 2m 58s\tremaining: 9m 40s\n",
      "235:\tlearn: 1917.4256741\ttotal: 2m 59s\tremaining: 9m 39s\n",
      "236:\tlearn: 1916.8160920\ttotal: 2m 59s\tremaining: 9m 38s\n",
      "237:\tlearn: 1916.4543275\ttotal: 3m\tremaining: 9m 37s\n",
      "238:\tlearn: 1915.7180815\ttotal: 3m 1s\tremaining: 9m 37s\n",
      "239:\tlearn: 1915.2386816\ttotal: 3m 1s\tremaining: 9m 36s\n",
      "240:\tlearn: 1914.6123151\ttotal: 3m 2s\tremaining: 9m 35s\n",
      "241:\tlearn: 1913.9906006\ttotal: 3m 3s\tremaining: 9m 34s\n",
      "242:\tlearn: 1913.4412449\ttotal: 3m 4s\tremaining: 9m 33s\n",
      "243:\tlearn: 1912.9489722\ttotal: 3m 4s\tremaining: 9m 33s\n",
      "244:\tlearn: 1912.4773494\ttotal: 3m 5s\tremaining: 9m 32s\n",
      "245:\tlearn: 1911.9789482\ttotal: 3m 6s\tremaining: 9m 31s\n",
      "246:\tlearn: 1911.3547656\ttotal: 3m 7s\tremaining: 9m 30s\n",
      "247:\tlearn: 1910.9839613\ttotal: 3m 8s\tremaining: 9m 30s\n",
      "248:\tlearn: 1910.5692781\ttotal: 3m 8s\tremaining: 9m 29s\n",
      "249:\tlearn: 1909.9843272\ttotal: 3m 9s\tremaining: 9m 29s\n",
      "250:\tlearn: 1909.6663900\ttotal: 3m 10s\tremaining: 9m 28s\n",
      "251:\tlearn: 1909.0512865\ttotal: 3m 11s\tremaining: 9m 28s\n",
      "252:\tlearn: 1908.4272016\ttotal: 3m 12s\tremaining: 9m 27s\n",
      "253:\tlearn: 1907.7700680\ttotal: 3m 13s\tremaining: 9m 27s\n",
      "254:\tlearn: 1907.2728885\ttotal: 3m 13s\tremaining: 9m 26s\n",
      "255:\tlearn: 1906.4321798\ttotal: 3m 14s\tremaining: 9m 25s\n",
      "256:\tlearn: 1905.8015353\ttotal: 3m 15s\tremaining: 9m 25s\n",
      "257:\tlearn: 1904.8498141\ttotal: 3m 16s\tremaining: 9m 24s\n",
      "258:\tlearn: 1904.5846995\ttotal: 3m 16s\tremaining: 9m 23s\n",
      "259:\tlearn: 1904.1594133\ttotal: 3m 17s\tremaining: 9m 22s\n",
      "260:\tlearn: 1903.6376147\ttotal: 3m 18s\tremaining: 9m 22s\n",
      "261:\tlearn: 1903.0153132\ttotal: 3m 19s\tremaining: 9m 21s\n",
      "262:\tlearn: 1902.6076345\ttotal: 3m 20s\tremaining: 9m 21s\n",
      "263:\tlearn: 1901.8816171\ttotal: 3m 20s\tremaining: 9m 19s\n",
      "264:\tlearn: 1901.1866013\ttotal: 3m 21s\tremaining: 9m 19s\n",
      "265:\tlearn: 1900.7505599\ttotal: 3m 22s\tremaining: 9m 18s\n",
      "266:\tlearn: 1900.3669329\ttotal: 3m 23s\tremaining: 9m 17s\n",
      "267:\tlearn: 1899.7099067\ttotal: 3m 23s\tremaining: 9m 16s\n",
      "268:\tlearn: 1899.3634079\ttotal: 3m 24s\tremaining: 9m 16s\n",
      "269:\tlearn: 1898.6894112\ttotal: 3m 25s\tremaining: 9m 14s\n",
      "270:\tlearn: 1898.2885851\ttotal: 3m 26s\tremaining: 9m 14s\n",
      "271:\tlearn: 1897.8213764\ttotal: 3m 26s\tremaining: 9m 13s\n",
      "272:\tlearn: 1897.0111521\ttotal: 3m 27s\tremaining: 9m 12s\n",
      "273:\tlearn: 1896.6077645\ttotal: 3m 28s\tremaining: 9m 12s\n",
      "274:\tlearn: 1896.1354717\ttotal: 3m 29s\tremaining: 9m 11s\n",
      "275:\tlearn: 1895.6826421\ttotal: 3m 29s\tremaining: 9m 10s\n",
      "276:\tlearn: 1895.3522118\ttotal: 3m 30s\tremaining: 9m 10s\n",
      "277:\tlearn: 1894.9799873\ttotal: 3m 31s\tremaining: 9m 9s\n",
      "278:\tlearn: 1894.4050238\ttotal: 3m 32s\tremaining: 9m 8s\n",
      "279:\tlearn: 1894.1788651\ttotal: 3m 33s\tremaining: 9m 7s\n",
      "280:\tlearn: 1893.7788639\ttotal: 3m 33s\tremaining: 9m 7s\n",
      "281:\tlearn: 1893.3355170\ttotal: 3m 34s\tremaining: 9m 6s\n",
      "282:\tlearn: 1892.9184178\ttotal: 3m 35s\tremaining: 9m 5s\n",
      "283:\tlearn: 1892.7465997\ttotal: 3m 36s\tremaining: 9m 4s\n",
      "284:\tlearn: 1892.1756216\ttotal: 3m 36s\tremaining: 9m 4s\n",
      "285:\tlearn: 1891.8138177\ttotal: 3m 37s\tremaining: 9m 3s\n",
      "286:\tlearn: 1891.4214412\ttotal: 3m 38s\tremaining: 9m 2s\n",
      "287:\tlearn: 1891.0647858\ttotal: 3m 39s\tremaining: 9m 2s\n",
      "288:\tlearn: 1890.6176552\ttotal: 3m 40s\tremaining: 9m 1s\n",
      "289:\tlearn: 1890.4313679\ttotal: 3m 40s\tremaining: 9m\n",
      "290:\tlearn: 1889.8065445\ttotal: 3m 41s\tremaining: 8m 59s\n",
      "291:\tlearn: 1889.3765722\ttotal: 3m 42s\tremaining: 8m 58s\n",
      "292:\tlearn: 1889.0455099\ttotal: 3m 42s\tremaining: 8m 57s\n",
      "293:\tlearn: 1888.4169501\ttotal: 3m 43s\tremaining: 8m 56s\n",
      "294:\tlearn: 1887.9434784\ttotal: 3m 44s\tremaining: 8m 56s\n",
      "295:\tlearn: 1887.2385593\ttotal: 3m 45s\tremaining: 8m 55s\n",
      "296:\tlearn: 1886.8816948\ttotal: 3m 45s\tremaining: 8m 54s\n",
      "297:\tlearn: 1886.3185696\ttotal: 3m 46s\tremaining: 8m 54s\n",
      "298:\tlearn: 1885.9919236\ttotal: 3m 47s\tremaining: 8m 52s\n",
      "299:\tlearn: 1885.5222589\ttotal: 3m 47s\tremaining: 8m 51s\n",
      "300:\tlearn: 1885.1788789\ttotal: 3m 48s\tremaining: 8m 50s\n",
      "301:\tlearn: 1884.8983841\ttotal: 3m 49s\tremaining: 8m 49s\n",
      "302:\tlearn: 1884.6296417\ttotal: 3m 49s\tremaining: 8m 48s\n",
      "303:\tlearn: 1884.3825640\ttotal: 3m 50s\tremaining: 8m 48s\n",
      "304:\tlearn: 1883.7665246\ttotal: 3m 51s\tremaining: 8m 47s\n",
      "305:\tlearn: 1883.5231216\ttotal: 3m 52s\tremaining: 8m 46s\n",
      "306:\tlearn: 1883.2731700\ttotal: 3m 53s\tremaining: 8m 46s\n",
      "307:\tlearn: 1882.9579228\ttotal: 3m 53s\tremaining: 8m 45s\n",
      "308:\tlearn: 1882.5723951\ttotal: 3m 54s\tremaining: 8m 44s\n",
      "309:\tlearn: 1882.3749628\ttotal: 3m 55s\tremaining: 8m 43s\n",
      "310:\tlearn: 1882.0005250\ttotal: 3m 56s\tremaining: 8m 43s\n",
      "311:\tlearn: 1881.7242557\ttotal: 3m 57s\tremaining: 8m 42s\n",
      "312:\tlearn: 1881.2206263\ttotal: 3m 57s\tremaining: 8m 41s\n",
      "313:\tlearn: 1880.8378966\ttotal: 3m 58s\tremaining: 8m 41s\n",
      "314:\tlearn: 1880.5097610\ttotal: 3m 59s\tremaining: 8m 40s\n",
      "315:\tlearn: 1880.0675306\ttotal: 4m\tremaining: 8m 39s\n",
      "316:\tlearn: 1879.8350371\ttotal: 4m 1s\tremaining: 8m 39s\n",
      "317:\tlearn: 1879.4000179\ttotal: 4m 1s\tremaining: 8m 38s\n",
      "318:\tlearn: 1878.8495927\ttotal: 4m 2s\tremaining: 8m 37s\n",
      "319:\tlearn: 1878.2816496\ttotal: 4m 3s\tremaining: 8m 36s\n",
      "320:\tlearn: 1877.7455051\ttotal: 4m 3s\tremaining: 8m 35s\n",
      "321:\tlearn: 1877.3808104\ttotal: 4m 4s\tremaining: 8m 35s\n",
      "322:\tlearn: 1876.9414624\ttotal: 4m 5s\tremaining: 8m 34s\n",
      "323:\tlearn: 1876.5485263\ttotal: 4m 6s\tremaining: 8m 34s\n",
      "324:\tlearn: 1876.0197533\ttotal: 4m 7s\tremaining: 8m 33s\n",
      "325:\tlearn: 1875.8252216\ttotal: 4m 7s\tremaining: 8m 32s\n",
      "326:\tlearn: 1875.4369729\ttotal: 4m 8s\tremaining: 8m 31s\n",
      "327:\tlearn: 1875.0756564\ttotal: 4m 9s\tremaining: 8m 30s\n",
      "328:\tlearn: 1874.6845657\ttotal: 4m 10s\tremaining: 8m 29s\n",
      "329:\tlearn: 1874.1183997\ttotal: 4m 10s\tremaining: 8m 28s\n",
      "330:\tlearn: 1873.9225284\ttotal: 4m 11s\tremaining: 8m 28s\n",
      "331:\tlearn: 1873.5640996\ttotal: 4m 12s\tremaining: 8m 27s\n",
      "332:\tlearn: 1873.2441854\ttotal: 4m 13s\tremaining: 8m 26s\n",
      "333:\tlearn: 1872.7541203\ttotal: 4m 13s\tremaining: 8m 26s\n",
      "334:\tlearn: 1872.4550222\ttotal: 4m 14s\tremaining: 8m 24s\n",
      "335:\tlearn: 1871.8959676\ttotal: 4m 14s\tremaining: 8m 23s\n",
      "336:\tlearn: 1871.6135948\ttotal: 4m 15s\tremaining: 8m 22s\n",
      "337:\tlearn: 1871.1348460\ttotal: 4m 16s\tremaining: 8m 21s\n",
      "338:\tlearn: 1870.7861350\ttotal: 4m 16s\tremaining: 8m 20s\n",
      "339:\tlearn: 1870.1524145\ttotal: 4m 17s\tremaining: 8m 19s\n",
      "340:\tlearn: 1869.8768486\ttotal: 4m 18s\tremaining: 8m 19s\n",
      "341:\tlearn: 1869.5574317\ttotal: 4m 19s\tremaining: 8m 18s\n",
      "342:\tlearn: 1869.2529095\ttotal: 4m 19s\tremaining: 8m 17s\n",
      "343:\tlearn: 1868.8513876\ttotal: 4m 20s\tremaining: 8m 17s\n",
      "344:\tlearn: 1868.3152237\ttotal: 4m 21s\tremaining: 8m 16s\n",
      "345:\tlearn: 1868.1697731\ttotal: 4m 22s\tremaining: 8m 15s\n",
      "346:\tlearn: 1867.5988204\ttotal: 4m 22s\tremaining: 8m 14s\n",
      "347:\tlearn: 1867.3705271\ttotal: 4m 23s\tremaining: 8m 13s\n",
      "348:\tlearn: 1866.9912500\ttotal: 4m 24s\tremaining: 8m 12s\n",
      "349:\tlearn: 1866.6692670\ttotal: 4m 24s\tremaining: 8m 11s\n",
      "350:\tlearn: 1866.4792640\ttotal: 4m 25s\tremaining: 8m 11s\n",
      "351:\tlearn: 1866.2789941\ttotal: 4m 26s\tremaining: 8m 10s\n",
      "352:\tlearn: 1866.0125966\ttotal: 4m 27s\tremaining: 8m 9s\n",
      "353:\tlearn: 1865.4324620\ttotal: 4m 27s\tremaining: 8m 8s\n",
      "354:\tlearn: 1865.2996578\ttotal: 4m 28s\tremaining: 8m 7s\n",
      "355:\tlearn: 1864.9696426\ttotal: 4m 29s\tremaining: 8m 7s\n",
      "356:\tlearn: 1864.7574361\ttotal: 4m 29s\tremaining: 8m 6s\n",
      "357:\tlearn: 1864.3842520\ttotal: 4m 30s\tremaining: 8m 5s\n",
      "358:\tlearn: 1863.8144921\ttotal: 4m 31s\tremaining: 8m 4s\n",
      "359:\tlearn: 1863.3913989\ttotal: 4m 32s\tremaining: 8m 3s\n",
      "360:\tlearn: 1863.0506440\ttotal: 4m 32s\tremaining: 8m 2s\n",
      "361:\tlearn: 1862.8148141\ttotal: 4m 33s\tremaining: 8m 2s\n",
      "362:\tlearn: 1862.6690369\ttotal: 4m 34s\tremaining: 8m 1s\n",
      "363:\tlearn: 1862.2651879\ttotal: 4m 35s\tremaining: 8m\n",
      "364:\tlearn: 1862.0760923\ttotal: 4m 36s\tremaining: 8m\n",
      "365:\tlearn: 1861.8290925\ttotal: 4m 36s\tremaining: 7m 59s\n",
      "366:\tlearn: 1861.6484872\ttotal: 4m 37s\tremaining: 7m 58s\n",
      "367:\tlearn: 1861.4326417\ttotal: 4m 38s\tremaining: 7m 58s\n",
      "368:\tlearn: 1860.9840929\ttotal: 4m 39s\tremaining: 7m 57s\n",
      "369:\tlearn: 1860.7644413\ttotal: 4m 39s\tremaining: 7m 56s\n",
      "370:\tlearn: 1860.4946035\ttotal: 4m 40s\tremaining: 7m 55s\n",
      "371:\tlearn: 1860.1063470\ttotal: 4m 41s\tremaining: 7m 55s\n",
      "372:\tlearn: 1859.7837286\ttotal: 4m 42s\tremaining: 7m 54s\n",
      "373:\tlearn: 1859.5942411\ttotal: 4m 43s\tremaining: 7m 53s\n",
      "374:\tlearn: 1858.8616991\ttotal: 4m 43s\tremaining: 7m 53s\n",
      "375:\tlearn: 1858.4966593\ttotal: 4m 44s\tremaining: 7m 52s\n",
      "376:\tlearn: 1858.3154021\ttotal: 4m 45s\tremaining: 7m 51s\n",
      "377:\tlearn: 1857.8400848\ttotal: 4m 46s\tremaining: 7m 51s\n",
      "378:\tlearn: 1857.5875743\ttotal: 4m 47s\tremaining: 7m 50s\n",
      "379:\tlearn: 1857.1452224\ttotal: 4m 48s\tremaining: 7m 49s\n",
      "380:\tlearn: 1856.8967405\ttotal: 4m 48s\tremaining: 7m 49s\n",
      "381:\tlearn: 1856.4682952\ttotal: 4m 49s\tremaining: 7m 48s\n",
      "382:\tlearn: 1856.2476458\ttotal: 4m 50s\tremaining: 7m 47s\n",
      "383:\tlearn: 1856.0644113\ttotal: 4m 51s\tremaining: 7m 47s\n",
      "384:\tlearn: 1855.9565362\ttotal: 4m 52s\tremaining: 7m 46s\n",
      "385:\tlearn: 1855.6502099\ttotal: 4m 52s\tremaining: 7m 45s\n",
      "386:\tlearn: 1855.3645912\ttotal: 4m 53s\tremaining: 7m 45s\n",
      "387:\tlearn: 1854.8596041\ttotal: 4m 54s\tremaining: 7m 44s\n",
      "388:\tlearn: 1854.2780231\ttotal: 4m 55s\tremaining: 7m 44s\n",
      "389:\tlearn: 1853.8376548\ttotal: 4m 56s\tremaining: 7m 43s\n",
      "390:\tlearn: 1853.5112197\ttotal: 4m 57s\tremaining: 7m 42s\n",
      "391:\tlearn: 1853.2957418\ttotal: 4m 57s\tremaining: 7m 42s\n",
      "392:\tlearn: 1853.0702996\ttotal: 4m 58s\tremaining: 7m 41s\n",
      "393:\tlearn: 1852.7110453\ttotal: 4m 59s\tremaining: 7m 40s\n",
      "394:\tlearn: 1852.4365078\ttotal: 5m\tremaining: 7m 40s\n",
      "395:\tlearn: 1852.2766467\ttotal: 5m 1s\tremaining: 7m 39s\n",
      "396:\tlearn: 1852.1357234\ttotal: 5m 2s\tremaining: 7m 38s\n",
      "397:\tlearn: 1851.8524792\ttotal: 5m 2s\tremaining: 7m 38s\n",
      "398:\tlearn: 1851.6998511\ttotal: 5m 3s\tremaining: 7m 37s\n",
      "399:\tlearn: 1851.3590092\ttotal: 5m 4s\tremaining: 7m 36s\n",
      "400:\tlearn: 1851.1651745\ttotal: 5m 5s\tremaining: 7m 35s\n",
      "401:\tlearn: 1850.6785947\ttotal: 5m 5s\tremaining: 7m 34s\n",
      "402:\tlearn: 1850.2496003\ttotal: 5m 6s\tremaining: 7m 34s\n",
      "403:\tlearn: 1849.9160506\ttotal: 5m 7s\tremaining: 7m 33s\n",
      "404:\tlearn: 1849.6692026\ttotal: 5m 7s\tremaining: 7m 32s\n",
      "405:\tlearn: 1849.4919649\ttotal: 5m 8s\tremaining: 7m 31s\n",
      "406:\tlearn: 1849.0339097\ttotal: 5m 9s\tremaining: 7m 31s\n",
      "407:\tlearn: 1848.7870086\ttotal: 5m 10s\tremaining: 7m 30s\n",
      "408:\tlearn: 1848.3775880\ttotal: 5m 11s\tremaining: 7m 29s\n",
      "409:\tlearn: 1848.2499345\ttotal: 5m 11s\tremaining: 7m 28s\n",
      "410:\tlearn: 1847.9881598\ttotal: 5m 12s\tremaining: 7m 28s\n",
      "411:\tlearn: 1847.5681624\ttotal: 5m 13s\tremaining: 7m 27s\n",
      "412:\tlearn: 1847.2996545\ttotal: 5m 14s\tremaining: 7m 26s\n",
      "413:\tlearn: 1846.9100439\ttotal: 5m 14s\tremaining: 7m 25s\n",
      "414:\tlearn: 1846.6417579\ttotal: 5m 15s\tremaining: 7m 25s\n",
      "415:\tlearn: 1846.2908968\ttotal: 5m 16s\tremaining: 7m 24s\n",
      "416:\tlearn: 1845.8340142\ttotal: 5m 17s\tremaining: 7m 23s\n",
      "417:\tlearn: 1845.6327874\ttotal: 5m 18s\tremaining: 7m 23s\n",
      "418:\tlearn: 1845.4146790\ttotal: 5m 19s\tremaining: 7m 22s\n",
      "419:\tlearn: 1845.0704152\ttotal: 5m 19s\tremaining: 7m 21s\n",
      "420:\tlearn: 1844.6156851\ttotal: 5m 20s\tremaining: 7m 21s\n",
      "421:\tlearn: 1844.4563604\ttotal: 5m 21s\tremaining: 7m 20s\n",
      "422:\tlearn: 1844.2348548\ttotal: 5m 22s\tremaining: 7m 19s\n",
      "423:\tlearn: 1843.7977728\ttotal: 5m 23s\tremaining: 7m 18s\n",
      "424:\tlearn: 1843.6586430\ttotal: 5m 23s\tremaining: 7m 18s\n",
      "425:\tlearn: 1843.4211911\ttotal: 5m 24s\tremaining: 7m 17s\n",
      "426:\tlearn: 1842.9693713\ttotal: 5m 25s\tremaining: 7m 16s\n",
      "427:\tlearn: 1842.6535811\ttotal: 5m 26s\tremaining: 7m 15s\n",
      "428:\tlearn: 1842.3094896\ttotal: 5m 26s\tremaining: 7m 15s\n",
      "429:\tlearn: 1842.1777078\ttotal: 5m 27s\tremaining: 7m 14s\n",
      "430:\tlearn: 1841.7912309\ttotal: 5m 28s\tremaining: 7m 13s\n",
      "431:\tlearn: 1841.4878709\ttotal: 5m 29s\tremaining: 7m 12s\n",
      "432:\tlearn: 1841.2230005\ttotal: 5m 29s\tremaining: 7m 12s\n",
      "433:\tlearn: 1840.8808926\ttotal: 5m 30s\tremaining: 7m 11s\n",
      "434:\tlearn: 1840.6537814\ttotal: 5m 31s\tremaining: 7m 10s\n",
      "435:\tlearn: 1840.3663399\ttotal: 5m 32s\tremaining: 7m 9s\n",
      "436:\tlearn: 1840.0607622\ttotal: 5m 32s\tremaining: 7m 8s\n",
      "437:\tlearn: 1839.9152299\ttotal: 5m 33s\tremaining: 7m 8s\n",
      "438:\tlearn: 1839.7478925\ttotal: 5m 34s\tremaining: 7m 7s\n",
      "439:\tlearn: 1839.3901498\ttotal: 5m 35s\tremaining: 7m 6s\n",
      "440:\tlearn: 1839.0827600\ttotal: 5m 35s\tremaining: 7m 5s\n",
      "441:\tlearn: 1838.7268236\ttotal: 5m 36s\tremaining: 7m 5s\n",
      "442:\tlearn: 1838.4969383\ttotal: 5m 37s\tremaining: 7m 4s\n",
      "443:\tlearn: 1838.3404796\ttotal: 5m 38s\tremaining: 7m 3s\n",
      "444:\tlearn: 1837.8797305\ttotal: 5m 38s\tremaining: 7m 2s\n",
      "445:\tlearn: 1837.5540323\ttotal: 5m 39s\tremaining: 7m 2s\n",
      "446:\tlearn: 1837.2854688\ttotal: 5m 40s\tremaining: 7m 1s\n",
      "447:\tlearn: 1836.9833053\ttotal: 5m 41s\tremaining: 7m\n",
      "448:\tlearn: 1836.5582388\ttotal: 5m 42s\tremaining: 7m\n",
      "449:\tlearn: 1836.2510024\ttotal: 5m 43s\tremaining: 6m 59s\n",
      "450:\tlearn: 1836.1131060\ttotal: 5m 43s\tremaining: 6m 58s\n",
      "451:\tlearn: 1835.8097053\ttotal: 5m 44s\tremaining: 6m 57s\n",
      "452:\tlearn: 1835.4726455\ttotal: 5m 45s\tremaining: 6m 57s\n",
      "453:\tlearn: 1835.1719934\ttotal: 5m 46s\tremaining: 6m 56s\n",
      "454:\tlearn: 1834.8108616\ttotal: 5m 47s\tremaining: 6m 55s\n",
      "455:\tlearn: 1834.7105632\ttotal: 5m 47s\tremaining: 6m 55s\n",
      "456:\tlearn: 1834.4058251\ttotal: 5m 48s\tremaining: 6m 54s\n",
      "457:\tlearn: 1834.0201529\ttotal: 5m 49s\tremaining: 6m 53s\n",
      "458:\tlearn: 1833.8277936\ttotal: 5m 50s\tremaining: 6m 52s\n",
      "459:\tlearn: 1833.7271770\ttotal: 5m 51s\tremaining: 6m 52s\n",
      "460:\tlearn: 1833.5213256\ttotal: 5m 51s\tremaining: 6m 51s\n",
      "461:\tlearn: 1833.3682003\ttotal: 5m 52s\tremaining: 6m 50s\n",
      "462:\tlearn: 1833.2082373\ttotal: 5m 53s\tremaining: 6m 50s\n",
      "463:\tlearn: 1832.8433274\ttotal: 5m 54s\tremaining: 6m 49s\n",
      "464:\tlearn: 1832.5442076\ttotal: 5m 55s\tremaining: 6m 48s\n",
      "465:\tlearn: 1832.1683650\ttotal: 5m 56s\tremaining: 6m 48s\n",
      "466:\tlearn: 1831.8221486\ttotal: 5m 56s\tremaining: 6m 47s\n",
      "467:\tlearn: 1831.6359210\ttotal: 5m 57s\tremaining: 6m 46s\n",
      "468:\tlearn: 1831.5005104\ttotal: 5m 58s\tremaining: 6m 45s\n",
      "469:\tlearn: 1831.3551152\ttotal: 5m 59s\tremaining: 6m 45s\n",
      "470:\tlearn: 1831.1051436\ttotal: 6m\tremaining: 6m 44s\n",
      "471:\tlearn: 1830.8211386\ttotal: 6m\tremaining: 6m 43s\n",
      "472:\tlearn: 1830.6548023\ttotal: 6m 1s\tremaining: 6m 42s\n",
      "473:\tlearn: 1830.5655080\ttotal: 6m 2s\tremaining: 6m 42s\n",
      "474:\tlearn: 1830.4131902\ttotal: 6m 3s\tremaining: 6m 41s\n",
      "475:\tlearn: 1830.1617007\ttotal: 6m 3s\tremaining: 6m 40s\n",
      "476:\tlearn: 1829.8216469\ttotal: 6m 4s\tremaining: 6m 40s\n",
      "477:\tlearn: 1829.6943379\ttotal: 6m 5s\tremaining: 6m 39s\n",
      "478:\tlearn: 1829.3890132\ttotal: 6m 6s\tremaining: 6m 38s\n",
      "479:\tlearn: 1829.1098529\ttotal: 6m 7s\tremaining: 6m 37s\n",
      "480:\tlearn: 1828.7343712\ttotal: 6m 8s\tremaining: 6m 37s\n",
      "481:\tlearn: 1828.4544165\ttotal: 6m 8s\tremaining: 6m 36s\n",
      "482:\tlearn: 1828.3116361\ttotal: 6m 9s\tremaining: 6m 35s\n",
      "483:\tlearn: 1828.1683845\ttotal: 6m 10s\tremaining: 6m 35s\n",
      "484:\tlearn: 1828.0315240\ttotal: 6m 11s\tremaining: 6m 33s\n",
      "485:\tlearn: 1827.8400566\ttotal: 6m 11s\tremaining: 6m 33s\n",
      "486:\tlearn: 1827.6560568\ttotal: 6m 12s\tremaining: 6m 32s\n",
      "487:\tlearn: 1827.5451196\ttotal: 6m 13s\tremaining: 6m 31s\n",
      "488:\tlearn: 1827.3982645\ttotal: 6m 14s\tremaining: 6m 31s\n",
      "489:\tlearn: 1827.1160604\ttotal: 6m 15s\tremaining: 6m 30s\n",
      "490:\tlearn: 1826.9950645\ttotal: 6m 16s\tremaining: 6m 29s\n",
      "491:\tlearn: 1826.8622585\ttotal: 6m 16s\tremaining: 6m 28s\n",
      "492:\tlearn: 1826.5210527\ttotal: 6m 17s\tremaining: 6m 28s\n",
      "493:\tlearn: 1826.1616149\ttotal: 6m 18s\tremaining: 6m 27s\n",
      "494:\tlearn: 1825.9387734\ttotal: 6m 19s\tremaining: 6m 26s\n",
      "495:\tlearn: 1825.7980416\ttotal: 6m 19s\tremaining: 6m 25s\n",
      "496:\tlearn: 1825.4521902\ttotal: 6m 20s\tremaining: 6m 25s\n",
      "497:\tlearn: 1825.3190284\ttotal: 6m 21s\tremaining: 6m 24s\n",
      "498:\tlearn: 1825.1379242\ttotal: 6m 22s\tremaining: 6m 23s\n",
      "499:\tlearn: 1824.9308582\ttotal: 6m 23s\tremaining: 6m 23s\n",
      "500:\tlearn: 1824.6967719\ttotal: 6m 23s\tremaining: 6m 22s\n",
      "501:\tlearn: 1824.5118954\ttotal: 6m 24s\tremaining: 6m 21s\n",
      "502:\tlearn: 1824.2599854\ttotal: 6m 25s\tremaining: 6m 20s\n",
      "503:\tlearn: 1824.0977132\ttotal: 6m 26s\tremaining: 6m 19s\n",
      "504:\tlearn: 1824.0349647\ttotal: 6m 26s\tremaining: 6m 19s\n",
      "505:\tlearn: 1823.9006636\ttotal: 6m 27s\tremaining: 6m 18s\n",
      "506:\tlearn: 1823.8155859\ttotal: 6m 28s\tremaining: 6m 17s\n",
      "507:\tlearn: 1823.5354769\ttotal: 6m 29s\tremaining: 6m 17s\n",
      "508:\tlearn: 1823.3617945\ttotal: 6m 30s\tremaining: 6m 16s\n",
      "509:\tlearn: 1823.1419427\ttotal: 6m 30s\tremaining: 6m 15s\n",
      "510:\tlearn: 1822.8668850\ttotal: 6m 31s\tremaining: 6m 14s\n",
      "511:\tlearn: 1822.6942723\ttotal: 6m 32s\tremaining: 6m 13s\n",
      "512:\tlearn: 1822.4728980\ttotal: 6m 33s\tremaining: 6m 13s\n",
      "513:\tlearn: 1822.0764957\ttotal: 6m 33s\tremaining: 6m 12s\n",
      "514:\tlearn: 1821.7890025\ttotal: 6m 34s\tremaining: 6m 11s\n",
      "515:\tlearn: 1821.6633878\ttotal: 6m 35s\tremaining: 6m 10s\n",
      "516:\tlearn: 1821.2986043\ttotal: 6m 36s\tremaining: 6m 10s\n",
      "517:\tlearn: 1820.9369951\ttotal: 6m 36s\tremaining: 6m 9s\n",
      "518:\tlearn: 1820.8125316\ttotal: 6m 37s\tremaining: 6m 8s\n",
      "519:\tlearn: 1820.6139408\ttotal: 6m 38s\tremaining: 6m 7s\n",
      "520:\tlearn: 1820.4950761\ttotal: 6m 39s\tremaining: 6m 7s\n",
      "521:\tlearn: 1820.3138123\ttotal: 6m 40s\tremaining: 6m 6s\n",
      "522:\tlearn: 1820.1702896\ttotal: 6m 40s\tremaining: 6m 5s\n",
      "523:\tlearn: 1819.8557784\ttotal: 6m 41s\tremaining: 6m 4s\n",
      "524:\tlearn: 1819.6614381\ttotal: 6m 42s\tremaining: 6m 4s\n",
      "525:\tlearn: 1819.4954840\ttotal: 6m 43s\tremaining: 6m 3s\n",
      "526:\tlearn: 1819.1851953\ttotal: 6m 43s\tremaining: 6m 2s\n",
      "527:\tlearn: 1819.0350557\ttotal: 6m 44s\tremaining: 6m 1s\n",
      "528:\tlearn: 1818.7392721\ttotal: 6m 45s\tremaining: 6m\n",
      "529:\tlearn: 1818.5801786\ttotal: 6m 46s\tremaining: 6m\n",
      "530:\tlearn: 1818.4744392\ttotal: 6m 47s\tremaining: 5m 59s\n",
      "531:\tlearn: 1818.2261576\ttotal: 6m 47s\tremaining: 5m 58s\n",
      "532:\tlearn: 1818.1030970\ttotal: 6m 48s\tremaining: 5m 57s\n",
      "533:\tlearn: 1817.8281823\ttotal: 6m 49s\tremaining: 5m 57s\n",
      "534:\tlearn: 1817.5942305\ttotal: 6m 50s\tremaining: 5m 56s\n",
      "535:\tlearn: 1817.4646025\ttotal: 6m 51s\tremaining: 5m 55s\n",
      "536:\tlearn: 1817.3968696\ttotal: 6m 51s\tremaining: 5m 55s\n",
      "537:\tlearn: 1817.2519547\ttotal: 6m 52s\tremaining: 5m 54s\n",
      "538:\tlearn: 1817.0646484\ttotal: 6m 53s\tremaining: 5m 53s\n",
      "539:\tlearn: 1816.9892649\ttotal: 6m 54s\tremaining: 5m 52s\n",
      "540:\tlearn: 1816.7041525\ttotal: 6m 54s\tremaining: 5m 51s\n",
      "541:\tlearn: 1816.5198062\ttotal: 6m 55s\tremaining: 5m 51s\n",
      "542:\tlearn: 1816.4442801\ttotal: 6m 56s\tremaining: 5m 50s\n",
      "543:\tlearn: 1816.3120770\ttotal: 6m 57s\tremaining: 5m 49s\n",
      "544:\tlearn: 1816.2408069\ttotal: 6m 57s\tremaining: 5m 48s\n",
      "545:\tlearn: 1815.9458929\ttotal: 6m 58s\tremaining: 5m 47s\n",
      "546:\tlearn: 1815.8671030\ttotal: 6m 59s\tremaining: 5m 47s\n",
      "547:\tlearn: 1815.6969555\ttotal: 7m\tremaining: 5m 46s\n",
      "548:\tlearn: 1815.3879640\ttotal: 7m\tremaining: 5m 45s\n",
      "549:\tlearn: 1815.0951427\ttotal: 7m 1s\tremaining: 5m 45s\n",
      "550:\tlearn: 1814.7363726\ttotal: 7m 2s\tremaining: 5m 44s\n",
      "551:\tlearn: 1814.6007185\ttotal: 7m 2s\tremaining: 5m 43s\n",
      "552:\tlearn: 1814.3955798\ttotal: 7m 3s\tremaining: 5m 42s\n",
      "553:\tlearn: 1814.1719479\ttotal: 7m 4s\tremaining: 5m 41s\n",
      "554:\tlearn: 1813.9779937\ttotal: 7m 5s\tremaining: 5m 41s\n",
      "555:\tlearn: 1813.8068891\ttotal: 7m 6s\tremaining: 5m 40s\n",
      "556:\tlearn: 1813.6303219\ttotal: 7m 6s\tremaining: 5m 39s\n",
      "557:\tlearn: 1813.5333202\ttotal: 7m 7s\tremaining: 5m 38s\n",
      "558:\tlearn: 1813.3665454\ttotal: 7m 8s\tremaining: 5m 37s\n",
      "559:\tlearn: 1813.0925429\ttotal: 7m 9s\tremaining: 5m 37s\n",
      "560:\tlearn: 1812.9908093\ttotal: 7m 10s\tremaining: 5m 36s\n",
      "561:\tlearn: 1812.8105155\ttotal: 7m 10s\tremaining: 5m 35s\n",
      "562:\tlearn: 1812.6615445\ttotal: 7m 11s\tremaining: 5m 34s\n",
      "563:\tlearn: 1812.4984844\ttotal: 7m 12s\tremaining: 5m 34s\n",
      "564:\tlearn: 1812.4073051\ttotal: 7m 12s\tremaining: 5m 33s\n",
      "565:\tlearn: 1812.2868852\ttotal: 7m 13s\tremaining: 5m 32s\n",
      "566:\tlearn: 1812.1022099\ttotal: 7m 14s\tremaining: 5m 31s\n",
      "567:\tlearn: 1811.9353951\ttotal: 7m 15s\tremaining: 5m 31s\n",
      "568:\tlearn: 1811.6762717\ttotal: 7m 15s\tremaining: 5m 30s\n",
      "569:\tlearn: 1811.5146045\ttotal: 7m 16s\tremaining: 5m 29s\n",
      "570:\tlearn: 1811.2458973\ttotal: 7m 17s\tremaining: 5m 28s\n",
      "571:\tlearn: 1811.1820337\ttotal: 7m 18s\tremaining: 5m 27s\n",
      "572:\tlearn: 1811.0577723\ttotal: 7m 18s\tremaining: 5m 27s\n",
      "573:\tlearn: 1810.8432028\ttotal: 7m 19s\tremaining: 5m 26s\n",
      "574:\tlearn: 1810.5779287\ttotal: 7m 20s\tremaining: 5m 25s\n",
      "575:\tlearn: 1810.4102812\ttotal: 7m 21s\tremaining: 5m 24s\n",
      "576:\tlearn: 1810.2715940\ttotal: 7m 21s\tremaining: 5m 23s\n",
      "577:\tlearn: 1810.1601597\ttotal: 7m 22s\tremaining: 5m 23s\n",
      "578:\tlearn: 1809.9617194\ttotal: 7m 23s\tremaining: 5m 22s\n",
      "579:\tlearn: 1809.8724562\ttotal: 7m 24s\tremaining: 5m 21s\n",
      "580:\tlearn: 1809.6387403\ttotal: 7m 24s\tremaining: 5m 20s\n",
      "581:\tlearn: 1809.4776424\ttotal: 7m 25s\tremaining: 5m 20s\n",
      "582:\tlearn: 1809.2080145\ttotal: 7m 26s\tremaining: 5m 19s\n",
      "583:\tlearn: 1809.0704130\ttotal: 7m 27s\tremaining: 5m 18s\n",
      "584:\tlearn: 1808.7702607\ttotal: 7m 27s\tremaining: 5m 17s\n",
      "585:\tlearn: 1808.5276348\ttotal: 7m 28s\tremaining: 5m 17s\n",
      "586:\tlearn: 1808.2238016\ttotal: 7m 29s\tremaining: 5m 16s\n",
      "587:\tlearn: 1808.0937742\ttotal: 7m 30s\tremaining: 5m 15s\n",
      "588:\tlearn: 1807.9247652\ttotal: 7m 31s\tremaining: 5m 14s\n",
      "589:\tlearn: 1807.7618696\ttotal: 7m 31s\tremaining: 5m 13s\n",
      "590:\tlearn: 1807.6210299\ttotal: 7m 32s\tremaining: 5m 13s\n",
      "591:\tlearn: 1807.4870682\ttotal: 7m 33s\tremaining: 5m 12s\n",
      "592:\tlearn: 1807.2858455\ttotal: 7m 34s\tremaining: 5m 11s\n",
      "593:\tlearn: 1807.1540975\ttotal: 7m 34s\tremaining: 5m 10s\n",
      "594:\tlearn: 1806.9031557\ttotal: 7m 35s\tremaining: 5m 10s\n",
      "595:\tlearn: 1806.7507689\ttotal: 7m 36s\tremaining: 5m 9s\n",
      "596:\tlearn: 1806.4967665\ttotal: 7m 37s\tremaining: 5m 8s\n",
      "597:\tlearn: 1806.4157211\ttotal: 7m 38s\tremaining: 5m 8s\n",
      "598:\tlearn: 1806.2010614\ttotal: 7m 38s\tremaining: 5m 7s\n",
      "599:\tlearn: 1805.9935248\ttotal: 7m 39s\tremaining: 5m 6s\n",
      "600:\tlearn: 1805.7340439\ttotal: 7m 40s\tremaining: 5m 5s\n",
      "601:\tlearn: 1805.4433598\ttotal: 7m 41s\tremaining: 5m 4s\n",
      "602:\tlearn: 1805.2659680\ttotal: 7m 42s\tremaining: 5m 4s\n",
      "603:\tlearn: 1805.1274900\ttotal: 7m 42s\tremaining: 5m 3s\n",
      "604:\tlearn: 1805.0017522\ttotal: 7m 43s\tremaining: 5m 2s\n",
      "605:\tlearn: 1804.8584007\ttotal: 7m 44s\tremaining: 5m 1s\n",
      "606:\tlearn: 1804.5733124\ttotal: 7m 45s\tremaining: 5m 1s\n",
      "607:\tlearn: 1804.4165107\ttotal: 7m 46s\tremaining: 5m\n",
      "608:\tlearn: 1804.3094054\ttotal: 7m 46s\tremaining: 4m 59s\n",
      "609:\tlearn: 1804.0482282\ttotal: 7m 47s\tremaining: 4m 58s\n",
      "610:\tlearn: 1803.9314644\ttotal: 7m 48s\tremaining: 4m 58s\n",
      "611:\tlearn: 1803.7334615\ttotal: 7m 49s\tremaining: 4m 57s\n",
      "612:\tlearn: 1803.5343935\ttotal: 7m 50s\tremaining: 4m 56s\n",
      "613:\tlearn: 1803.2947871\ttotal: 7m 50s\tremaining: 4m 55s\n",
      "614:\tlearn: 1803.1602653\ttotal: 7m 51s\tremaining: 4m 55s\n",
      "615:\tlearn: 1802.9267288\ttotal: 7m 52s\tremaining: 4m 54s\n",
      "616:\tlearn: 1802.6933836\ttotal: 7m 52s\tremaining: 4m 53s\n",
      "617:\tlearn: 1802.4981616\ttotal: 7m 53s\tremaining: 4m 52s\n",
      "618:\tlearn: 1802.3833331\ttotal: 7m 54s\tremaining: 4m 51s\n",
      "619:\tlearn: 1802.1286846\ttotal: 7m 55s\tremaining: 4m 51s\n",
      "620:\tlearn: 1801.8316548\ttotal: 7m 56s\tremaining: 4m 50s\n",
      "621:\tlearn: 1801.7638244\ttotal: 7m 56s\tremaining: 4m 49s\n",
      "622:\tlearn: 1801.6901453\ttotal: 7m 57s\tremaining: 4m 49s\n",
      "623:\tlearn: 1801.3884546\ttotal: 7m 58s\tremaining: 4m 48s\n",
      "624:\tlearn: 1801.1993465\ttotal: 7m 59s\tremaining: 4m 47s\n",
      "625:\tlearn: 1801.0164932\ttotal: 8m\tremaining: 4m 46s\n",
      "626:\tlearn: 1800.8123775\ttotal: 8m\tremaining: 4m 46s\n",
      "627:\tlearn: 1800.6644452\ttotal: 8m 1s\tremaining: 4m 45s\n",
      "628:\tlearn: 1800.4886140\ttotal: 8m 2s\tremaining: 4m 44s\n",
      "629:\tlearn: 1800.3680349\ttotal: 8m 3s\tremaining: 4m 43s\n",
      "630:\tlearn: 1800.1585018\ttotal: 8m 3s\tremaining: 4m 42s\n",
      "631:\tlearn: 1800.0323922\ttotal: 8m 4s\tremaining: 4m 42s\n",
      "632:\tlearn: 1799.8784340\ttotal: 8m 5s\tremaining: 4m 41s\n",
      "633:\tlearn: 1799.7184978\ttotal: 8m 6s\tremaining: 4m 40s\n",
      "634:\tlearn: 1799.6410476\ttotal: 8m 6s\tremaining: 4m 39s\n",
      "635:\tlearn: 1799.5281220\ttotal: 8m 7s\tremaining: 4m 39s\n",
      "636:\tlearn: 1799.4481680\ttotal: 8m 8s\tremaining: 4m 38s\n",
      "637:\tlearn: 1799.3151914\ttotal: 8m 9s\tremaining: 4m 37s\n",
      "638:\tlearn: 1799.0413990\ttotal: 8m 10s\tremaining: 4m 36s\n",
      "639:\tlearn: 1798.8445192\ttotal: 8m 10s\tremaining: 4m 36s\n",
      "640:\tlearn: 1798.6900315\ttotal: 8m 11s\tremaining: 4m 35s\n",
      "641:\tlearn: 1798.6011420\ttotal: 8m 12s\tremaining: 4m 34s\n",
      "642:\tlearn: 1798.3133283\ttotal: 8m 13s\tremaining: 4m 33s\n",
      "643:\tlearn: 1798.1062150\ttotal: 8m 13s\tremaining: 4m 33s\n",
      "644:\tlearn: 1797.9173711\ttotal: 8m 14s\tremaining: 4m 32s\n",
      "645:\tlearn: 1797.8353331\ttotal: 8m 15s\tremaining: 4m 31s\n",
      "646:\tlearn: 1797.7482278\ttotal: 8m 16s\tremaining: 4m 30s\n",
      "647:\tlearn: 1797.6264285\ttotal: 8m 16s\tremaining: 4m 29s\n",
      "648:\tlearn: 1797.5180342\ttotal: 8m 17s\tremaining: 4m 29s\n",
      "649:\tlearn: 1797.3162971\ttotal: 8m 18s\tremaining: 4m 28s\n",
      "650:\tlearn: 1797.1560337\ttotal: 8m 18s\tremaining: 4m 27s\n",
      "651:\tlearn: 1796.9019487\ttotal: 8m 19s\tremaining: 4m 26s\n",
      "652:\tlearn: 1796.7262014\ttotal: 8m 20s\tremaining: 4m 25s\n",
      "653:\tlearn: 1796.6234791\ttotal: 8m 20s\tremaining: 4m 24s\n",
      "654:\tlearn: 1796.4867361\ttotal: 8m 21s\tremaining: 4m 24s\n",
      "655:\tlearn: 1796.3385206\ttotal: 8m 22s\tremaining: 4m 23s\n",
      "656:\tlearn: 1796.1388611\ttotal: 8m 23s\tremaining: 4m 22s\n",
      "657:\tlearn: 1796.0032429\ttotal: 8m 23s\tremaining: 4m 21s\n",
      "658:\tlearn: 1795.8760574\ttotal: 8m 24s\tremaining: 4m 21s\n",
      "659:\tlearn: 1795.7118632\ttotal: 8m 25s\tremaining: 4m 20s\n",
      "660:\tlearn: 1795.5598127\ttotal: 8m 26s\tremaining: 4m 19s\n",
      "661:\tlearn: 1795.4174026\ttotal: 8m 26s\tremaining: 4m 18s\n",
      "662:\tlearn: 1795.3387318\ttotal: 8m 27s\tremaining: 4m 17s\n",
      "663:\tlearn: 1795.1889488\ttotal: 8m 28s\tremaining: 4m 17s\n",
      "664:\tlearn: 1795.0243619\ttotal: 8m 28s\tremaining: 4m 16s\n",
      "665:\tlearn: 1794.8889139\ttotal: 8m 29s\tremaining: 4m 15s\n",
      "666:\tlearn: 1794.7892681\ttotal: 8m 30s\tremaining: 4m 14s\n",
      "667:\tlearn: 1794.6222118\ttotal: 8m 31s\tremaining: 4m 14s\n",
      "668:\tlearn: 1794.5283452\ttotal: 8m 31s\tremaining: 4m 13s\n",
      "669:\tlearn: 1794.3077021\ttotal: 8m 32s\tremaining: 4m 12s\n",
      "670:\tlearn: 1794.2052381\ttotal: 8m 33s\tremaining: 4m 11s\n",
      "671:\tlearn: 1794.1143817\ttotal: 8m 34s\tremaining: 4m 10s\n",
      "672:\tlearn: 1793.8749389\ttotal: 8m 34s\tremaining: 4m 10s\n",
      "673:\tlearn: 1793.7963070\ttotal: 8m 35s\tremaining: 4m 9s\n",
      "674:\tlearn: 1793.7363241\ttotal: 8m 36s\tremaining: 4m 8s\n",
      "675:\tlearn: 1793.6422163\ttotal: 8m 37s\tremaining: 4m 7s\n",
      "676:\tlearn: 1793.4952027\ttotal: 8m 37s\tremaining: 4m 7s\n",
      "677:\tlearn: 1793.3751951\ttotal: 8m 38s\tremaining: 4m 6s\n",
      "678:\tlearn: 1793.1632858\ttotal: 8m 39s\tremaining: 4m 5s\n",
      "679:\tlearn: 1793.0533374\ttotal: 8m 39s\tremaining: 4m 4s\n",
      "680:\tlearn: 1792.8426592\ttotal: 8m 40s\tremaining: 4m 3s\n",
      "681:\tlearn: 1792.6985071\ttotal: 8m 41s\tremaining: 4m 3s\n",
      "682:\tlearn: 1792.5051169\ttotal: 8m 42s\tremaining: 4m 2s\n",
      "683:\tlearn: 1792.3878784\ttotal: 8m 43s\tremaining: 4m 1s\n",
      "684:\tlearn: 1792.2069121\ttotal: 8m 43s\tremaining: 4m\n",
      "685:\tlearn: 1792.1551417\ttotal: 8m 44s\tremaining: 4m\n",
      "686:\tlearn: 1791.9730868\ttotal: 8m 45s\tremaining: 3m 59s\n",
      "687:\tlearn: 1791.8107829\ttotal: 8m 46s\tremaining: 3m 58s\n",
      "688:\tlearn: 1791.6640800\ttotal: 8m 46s\tremaining: 3m 57s\n",
      "689:\tlearn: 1791.5312767\ttotal: 8m 47s\tremaining: 3m 56s\n",
      "690:\tlearn: 1791.3793176\ttotal: 8m 48s\tremaining: 3m 56s\n",
      "691:\tlearn: 1791.2420461\ttotal: 8m 49s\tremaining: 3m 55s\n",
      "692:\tlearn: 1791.0073342\ttotal: 8m 49s\tremaining: 3m 54s\n",
      "693:\tlearn: 1790.8133970\ttotal: 8m 50s\tremaining: 3m 53s\n",
      "694:\tlearn: 1790.7510336\ttotal: 8m 51s\tremaining: 3m 53s\n",
      "695:\tlearn: 1790.5682638\ttotal: 8m 51s\tremaining: 3m 52s\n",
      "696:\tlearn: 1790.3521270\ttotal: 8m 52s\tremaining: 3m 51s\n",
      "697:\tlearn: 1790.1563123\ttotal: 8m 53s\tremaining: 3m 50s\n",
      "698:\tlearn: 1790.0337744\ttotal: 8m 53s\tremaining: 3m 49s\n",
      "699:\tlearn: 1789.9419073\ttotal: 8m 54s\tremaining: 3m 49s\n",
      "700:\tlearn: 1789.6957594\ttotal: 8m 55s\tremaining: 3m 48s\n",
      "701:\tlearn: 1789.5642305\ttotal: 8m 55s\tremaining: 3m 47s\n",
      "702:\tlearn: 1789.3823312\ttotal: 8m 56s\tremaining: 3m 46s\n",
      "703:\tlearn: 1789.3182510\ttotal: 8m 57s\tremaining: 3m 45s\n",
      "704:\tlearn: 1789.1256605\ttotal: 8m 58s\tremaining: 3m 45s\n",
      "705:\tlearn: 1789.0237772\ttotal: 8m 58s\tremaining: 3m 44s\n",
      "706:\tlearn: 1788.8651042\ttotal: 8m 59s\tremaining: 3m 43s\n",
      "707:\tlearn: 1788.7804765\ttotal: 9m\tremaining: 3m 42s\n",
      "708:\tlearn: 1788.5816797\ttotal: 9m\tremaining: 3m 42s\n",
      "709:\tlearn: 1788.4784412\ttotal: 9m 1s\tremaining: 3m 41s\n",
      "710:\tlearn: 1788.3965901\ttotal: 9m 2s\tremaining: 3m 40s\n",
      "711:\tlearn: 1788.2299507\ttotal: 9m 3s\tremaining: 3m 39s\n",
      "712:\tlearn: 1788.0371943\ttotal: 9m 4s\tremaining: 3m 39s\n",
      "713:\tlearn: 1787.9293539\ttotal: 9m 5s\tremaining: 3m 38s\n",
      "714:\tlearn: 1787.7693157\ttotal: 9m 5s\tremaining: 3m 37s\n",
      "715:\tlearn: 1787.6436789\ttotal: 9m 6s\tremaining: 3m 36s\n",
      "716:\tlearn: 1787.5571802\ttotal: 9m 7s\tremaining: 3m 36s\n",
      "717:\tlearn: 1787.4516446\ttotal: 9m 8s\tremaining: 3m 35s\n",
      "718:\tlearn: 1787.2616787\ttotal: 9m 8s\tremaining: 3m 34s\n",
      "719:\tlearn: 1787.1461498\ttotal: 9m 9s\tremaining: 3m 33s\n",
      "720:\tlearn: 1787.0491870\ttotal: 9m 10s\tremaining: 3m 33s\n",
      "721:\tlearn: 1786.9469055\ttotal: 9m 11s\tremaining: 3m 32s\n",
      "722:\tlearn: 1786.8912971\ttotal: 9m 11s\tremaining: 3m 31s\n",
      "723:\tlearn: 1786.7665259\ttotal: 9m 12s\tremaining: 3m 30s\n",
      "724:\tlearn: 1786.5760556\ttotal: 9m 13s\tremaining: 3m 29s\n",
      "725:\tlearn: 1786.4678683\ttotal: 9m 14s\tremaining: 3m 29s\n",
      "726:\tlearn: 1786.3586924\ttotal: 9m 14s\tremaining: 3m 28s\n",
      "727:\tlearn: 1786.2647197\ttotal: 9m 15s\tremaining: 3m 27s\n",
      "728:\tlearn: 1786.1586051\ttotal: 9m 16s\tremaining: 3m 26s\n",
      "729:\tlearn: 1786.0597918\ttotal: 9m 16s\tremaining: 3m 26s\n",
      "730:\tlearn: 1785.9329654\ttotal: 9m 17s\tremaining: 3m 25s\n",
      "731:\tlearn: 1785.9011102\ttotal: 9m 18s\tremaining: 3m 24s\n",
      "732:\tlearn: 1785.7878429\ttotal: 9m 19s\tremaining: 3m 23s\n",
      "733:\tlearn: 1785.6144301\ttotal: 9m 20s\tremaining: 3m 23s\n",
      "734:\tlearn: 1785.4742794\ttotal: 9m 20s\tremaining: 3m 22s\n",
      "735:\tlearn: 1785.3838194\ttotal: 9m 21s\tremaining: 3m 21s\n",
      "736:\tlearn: 1785.3462619\ttotal: 9m 22s\tremaining: 3m 20s\n",
      "737:\tlearn: 1785.1808088\ttotal: 9m 22s\tremaining: 3m 19s\n",
      "738:\tlearn: 1785.0679067\ttotal: 9m 23s\tremaining: 3m 19s\n",
      "739:\tlearn: 1784.9513458\ttotal: 9m 24s\tremaining: 3m 18s\n",
      "740:\tlearn: 1784.9041658\ttotal: 9m 25s\tremaining: 3m 17s\n",
      "741:\tlearn: 1784.7150650\ttotal: 9m 25s\tremaining: 3m 16s\n",
      "742:\tlearn: 1784.5911355\ttotal: 9m 26s\tremaining: 3m 15s\n",
      "743:\tlearn: 1784.4734530\ttotal: 9m 26s\tremaining: 3m 15s\n",
      "744:\tlearn: 1784.3707599\ttotal: 9m 27s\tremaining: 3m 14s\n",
      "745:\tlearn: 1784.3411962\ttotal: 9m 28s\tremaining: 3m 13s\n",
      "746:\tlearn: 1784.2702596\ttotal: 9m 28s\tremaining: 3m 12s\n",
      "747:\tlearn: 1784.0492818\ttotal: 9m 29s\tremaining: 3m 11s\n",
      "748:\tlearn: 1784.0042380\ttotal: 9m 30s\tremaining: 3m 11s\n",
      "749:\tlearn: 1783.8803098\ttotal: 9m 31s\tremaining: 3m 10s\n",
      "750:\tlearn: 1783.6718288\ttotal: 9m 32s\tremaining: 3m 9s\n",
      "751:\tlearn: 1783.4998088\ttotal: 9m 33s\tremaining: 3m 8s\n",
      "752:\tlearn: 1783.3311771\ttotal: 9m 33s\tremaining: 3m 8s\n",
      "753:\tlearn: 1783.2026673\ttotal: 9m 34s\tremaining: 3m 7s\n",
      "754:\tlearn: 1783.0479893\ttotal: 9m 35s\tremaining: 3m 6s\n",
      "755:\tlearn: 1782.8968179\ttotal: 9m 36s\tremaining: 3m 5s\n",
      "756:\tlearn: 1782.8155977\ttotal: 9m 36s\tremaining: 3m 5s\n",
      "757:\tlearn: 1782.6138821\ttotal: 9m 37s\tremaining: 3m 4s\n",
      "758:\tlearn: 1782.4733500\ttotal: 9m 38s\tremaining: 3m 3s\n",
      "759:\tlearn: 1782.3524735\ttotal: 9m 39s\tremaining: 3m 2s\n",
      "760:\tlearn: 1782.2520955\ttotal: 9m 40s\tremaining: 3m 2s\n",
      "761:\tlearn: 1782.1846257\ttotal: 9m 41s\tremaining: 3m 1s\n",
      "762:\tlearn: 1782.0452822\ttotal: 9m 41s\tremaining: 3m\n",
      "763:\tlearn: 1781.9687637\ttotal: 9m 42s\tremaining: 2m 59s\n",
      "764:\tlearn: 1781.8402791\ttotal: 9m 43s\tremaining: 2m 59s\n",
      "765:\tlearn: 1781.6938284\ttotal: 9m 44s\tremaining: 2m 58s\n",
      "766:\tlearn: 1781.5478782\ttotal: 9m 44s\tremaining: 2m 57s\n",
      "767:\tlearn: 1781.3603791\ttotal: 9m 45s\tremaining: 2m 56s\n",
      "768:\tlearn: 1781.1960291\ttotal: 9m 46s\tremaining: 2m 56s\n",
      "769:\tlearn: 1781.0456532\ttotal: 9m 47s\tremaining: 2m 55s\n",
      "770:\tlearn: 1780.8914487\ttotal: 9m 47s\tremaining: 2m 54s\n",
      "771:\tlearn: 1780.7808939\ttotal: 9m 48s\tremaining: 2m 53s\n",
      "772:\tlearn: 1780.5805229\ttotal: 9m 49s\tremaining: 2m 53s\n",
      "773:\tlearn: 1780.4938901\ttotal: 9m 49s\tremaining: 2m 52s\n",
      "774:\tlearn: 1780.3392389\ttotal: 9m 50s\tremaining: 2m 51s\n",
      "775:\tlearn: 1780.2031045\ttotal: 9m 51s\tremaining: 2m 50s\n",
      "776:\tlearn: 1780.0048562\ttotal: 9m 52s\tremaining: 2m 49s\n",
      "777:\tlearn: 1779.7809849\ttotal: 9m 52s\tremaining: 2m 49s\n",
      "778:\tlearn: 1779.7160873\ttotal: 9m 53s\tremaining: 2m 48s\n",
      "779:\tlearn: 1779.5507633\ttotal: 9m 54s\tremaining: 2m 47s\n",
      "780:\tlearn: 1779.4328526\ttotal: 9m 55s\tremaining: 2m 46s\n",
      "781:\tlearn: 1779.3443242\ttotal: 9m 55s\tremaining: 2m 46s\n",
      "782:\tlearn: 1779.3113422\ttotal: 9m 56s\tremaining: 2m 45s\n",
      "783:\tlearn: 1779.2050130\ttotal: 9m 57s\tremaining: 2m 44s\n",
      "784:\tlearn: 1779.1076540\ttotal: 9m 58s\tremaining: 2m 43s\n",
      "785:\tlearn: 1779.0322584\ttotal: 9m 58s\tremaining: 2m 42s\n",
      "786:\tlearn: 1778.9461777\ttotal: 9m 59s\tremaining: 2m 42s\n",
      "787:\tlearn: 1778.8464837\ttotal: 10m\tremaining: 2m 41s\n",
      "788:\tlearn: 1778.7286147\ttotal: 10m\tremaining: 2m 40s\n",
      "789:\tlearn: 1778.6266619\ttotal: 10m 1s\tremaining: 2m 39s\n",
      "790:\tlearn: 1778.4189422\ttotal: 10m 2s\tremaining: 2m 39s\n",
      "791:\tlearn: 1778.2381277\ttotal: 10m 3s\tremaining: 2m 38s\n",
      "792:\tlearn: 1778.1196022\ttotal: 10m 3s\tremaining: 2m 37s\n",
      "793:\tlearn: 1778.0882525\ttotal: 10m 4s\tremaining: 2m 36s\n",
      "794:\tlearn: 1778.0059501\ttotal: 10m 5s\tremaining: 2m 36s\n",
      "795:\tlearn: 1777.9364115\ttotal: 10m 5s\tremaining: 2m 35s\n",
      "796:\tlearn: 1777.7761804\ttotal: 10m 6s\tremaining: 2m 34s\n",
      "797:\tlearn: 1777.5606982\ttotal: 10m 7s\tremaining: 2m 33s\n",
      "798:\tlearn: 1777.4142831\ttotal: 10m 8s\tremaining: 2m 32s\n",
      "799:\tlearn: 1777.2640256\ttotal: 10m 8s\tremaining: 2m 32s\n",
      "800:\tlearn: 1777.1394306\ttotal: 10m 9s\tremaining: 2m 31s\n",
      "801:\tlearn: 1776.9999079\ttotal: 10m 10s\tremaining: 2m 30s\n",
      "802:\tlearn: 1776.8360606\ttotal: 10m 11s\tremaining: 2m 29s\n",
      "803:\tlearn: 1776.7456177\ttotal: 10m 11s\tremaining: 2m 29s\n",
      "804:\tlearn: 1776.6105738\ttotal: 10m 12s\tremaining: 2m 28s\n",
      "805:\tlearn: 1776.5505041\ttotal: 10m 13s\tremaining: 2m 27s\n",
      "806:\tlearn: 1776.3434862\ttotal: 10m 14s\tremaining: 2m 26s\n",
      "807:\tlearn: 1776.2523172\ttotal: 10m 15s\tremaining: 2m 26s\n",
      "808:\tlearn: 1776.1657681\ttotal: 10m 15s\tremaining: 2m 25s\n",
      "809:\tlearn: 1775.9394438\ttotal: 10m 16s\tremaining: 2m 24s\n",
      "810:\tlearn: 1775.7065646\ttotal: 10m 17s\tremaining: 2m 23s\n",
      "811:\tlearn: 1775.5547821\ttotal: 10m 18s\tremaining: 2m 23s\n",
      "812:\tlearn: 1775.3831077\ttotal: 10m 18s\tremaining: 2m 22s\n",
      "813:\tlearn: 1775.3164472\ttotal: 10m 19s\tremaining: 2m 21s\n",
      "814:\tlearn: 1775.2589741\ttotal: 10m 20s\tremaining: 2m 20s\n",
      "815:\tlearn: 1775.0310640\ttotal: 10m 21s\tremaining: 2m 20s\n",
      "816:\tlearn: 1774.9830192\ttotal: 10m 21s\tremaining: 2m 19s\n",
      "817:\tlearn: 1774.8564949\ttotal: 10m 22s\tremaining: 2m 18s\n",
      "818:\tlearn: 1774.7779129\ttotal: 10m 23s\tremaining: 2m 17s\n",
      "819:\tlearn: 1774.6417572\ttotal: 10m 24s\tremaining: 2m 17s\n",
      "820:\tlearn: 1774.5103428\ttotal: 10m 25s\tremaining: 2m 16s\n",
      "821:\tlearn: 1774.3784727\ttotal: 10m 25s\tremaining: 2m 15s\n",
      "822:\tlearn: 1774.1814817\ttotal: 10m 26s\tremaining: 2m 14s\n",
      "823:\tlearn: 1774.0992687\ttotal: 10m 27s\tremaining: 2m 14s\n",
      "824:\tlearn: 1773.9756381\ttotal: 10m 28s\tremaining: 2m 13s\n",
      "825:\tlearn: 1773.8961064\ttotal: 10m 29s\tremaining: 2m 12s\n",
      "826:\tlearn: 1773.7853921\ttotal: 10m 29s\tremaining: 2m 11s\n",
      "827:\tlearn: 1773.6495807\ttotal: 10m 30s\tremaining: 2m 11s\n",
      "828:\tlearn: 1773.5537951\ttotal: 10m 31s\tremaining: 2m 10s\n",
      "829:\tlearn: 1773.4680337\ttotal: 10m 32s\tremaining: 2m 9s\n",
      "830:\tlearn: 1773.2372430\ttotal: 10m 32s\tremaining: 2m 8s\n",
      "831:\tlearn: 1773.1756000\ttotal: 10m 33s\tremaining: 2m 7s\n",
      "832:\tlearn: 1772.9665244\ttotal: 10m 33s\tremaining: 2m 7s\n",
      "833:\tlearn: 1772.8510815\ttotal: 10m 34s\tremaining: 2m 6s\n",
      "834:\tlearn: 1772.7559130\ttotal: 10m 35s\tremaining: 2m 5s\n",
      "835:\tlearn: 1772.5465738\ttotal: 10m 36s\tremaining: 2m 4s\n",
      "836:\tlearn: 1772.4049018\ttotal: 10m 37s\tremaining: 2m 4s\n",
      "837:\tlearn: 1772.2061120\ttotal: 10m 37s\tremaining: 2m 3s\n",
      "838:\tlearn: 1771.9894785\ttotal: 10m 38s\tremaining: 2m 2s\n",
      "839:\tlearn: 1771.7819999\ttotal: 10m 39s\tremaining: 2m 1s\n",
      "840:\tlearn: 1771.7526520\ttotal: 10m 39s\tremaining: 2m\n",
      "841:\tlearn: 1771.5653543\ttotal: 10m 40s\tremaining: 2m\n",
      "842:\tlearn: 1771.4809381\ttotal: 10m 41s\tremaining: 1m 59s\n",
      "843:\tlearn: 1771.4027659\ttotal: 10m 41s\tremaining: 1m 58s\n",
      "844:\tlearn: 1771.2894428\ttotal: 10m 42s\tremaining: 1m 57s\n",
      "845:\tlearn: 1771.1383316\ttotal: 10m 43s\tremaining: 1m 57s\n",
      "846:\tlearn: 1771.0381015\ttotal: 10m 44s\tremaining: 1m 56s\n",
      "847:\tlearn: 1770.9515820\ttotal: 10m 44s\tremaining: 1m 55s\n",
      "848:\tlearn: 1770.7775298\ttotal: 10m 45s\tremaining: 1m 54s\n",
      "849:\tlearn: 1770.6535155\ttotal: 10m 46s\tremaining: 1m 54s\n",
      "850:\tlearn: 1770.4722496\ttotal: 10m 47s\tremaining: 1m 53s\n",
      "851:\tlearn: 1770.2866639\ttotal: 10m 47s\tremaining: 1m 52s\n",
      "852:\tlearn: 1770.1584659\ttotal: 10m 48s\tremaining: 1m 51s\n",
      "853:\tlearn: 1770.0839885\ttotal: 10m 49s\tremaining: 1m 51s\n",
      "854:\tlearn: 1770.0451747\ttotal: 10m 50s\tremaining: 1m 50s\n",
      "855:\tlearn: 1769.8673158\ttotal: 10m 51s\tremaining: 1m 49s\n",
      "856:\tlearn: 1769.8148301\ttotal: 10m 51s\tremaining: 1m 48s\n",
      "857:\tlearn: 1769.7311799\ttotal: 10m 52s\tremaining: 1m 47s\n",
      "858:\tlearn: 1769.6374647\ttotal: 10m 53s\tremaining: 1m 47s\n",
      "859:\tlearn: 1769.4429489\ttotal: 10m 53s\tremaining: 1m 46s\n",
      "860:\tlearn: 1769.3271222\ttotal: 10m 54s\tremaining: 1m 45s\n",
      "861:\tlearn: 1769.1982611\ttotal: 10m 55s\tremaining: 1m 44s\n",
      "862:\tlearn: 1769.1375538\ttotal: 10m 56s\tremaining: 1m 44s\n",
      "863:\tlearn: 1769.0094166\ttotal: 10m 56s\tremaining: 1m 43s\n",
      "864:\tlearn: 1768.9215663\ttotal: 10m 57s\tremaining: 1m 42s\n",
      "865:\tlearn: 1768.7023481\ttotal: 10m 58s\tremaining: 1m 41s\n",
      "866:\tlearn: 1768.5639232\ttotal: 10m 59s\tremaining: 1m 41s\n",
      "867:\tlearn: 1768.3992023\ttotal: 11m\tremaining: 1m 40s\n",
      "868:\tlearn: 1768.2918353\ttotal: 11m\tremaining: 1m 39s\n",
      "869:\tlearn: 1768.2242170\ttotal: 11m 1s\tremaining: 1m 38s\n",
      "870:\tlearn: 1768.1014500\ttotal: 11m 2s\tremaining: 1m 38s\n",
      "871:\tlearn: 1768.0383383\ttotal: 11m 2s\tremaining: 1m 37s\n",
      "872:\tlearn: 1767.9084272\ttotal: 11m 3s\tremaining: 1m 36s\n",
      "873:\tlearn: 1767.7885292\ttotal: 11m 4s\tremaining: 1m 35s\n",
      "874:\tlearn: 1767.7327583\ttotal: 11m 5s\tremaining: 1m 35s\n",
      "875:\tlearn: 1767.5873588\ttotal: 11m 5s\tremaining: 1m 34s\n",
      "876:\tlearn: 1767.5169287\ttotal: 11m 6s\tremaining: 1m 33s\n",
      "877:\tlearn: 1767.3986878\ttotal: 11m 7s\tremaining: 1m 32s\n",
      "878:\tlearn: 1767.2871978\ttotal: 11m 8s\tremaining: 1m 32s\n",
      "879:\tlearn: 1767.1769423\ttotal: 11m 9s\tremaining: 1m 31s\n",
      "880:\tlearn: 1767.0220868\ttotal: 11m 10s\tremaining: 1m 30s\n",
      "881:\tlearn: 1766.9232453\ttotal: 11m 11s\tremaining: 1m 29s\n",
      "882:\tlearn: 1766.7819808\ttotal: 11m 11s\tremaining: 1m 28s\n",
      "883:\tlearn: 1766.6829563\ttotal: 11m 12s\tremaining: 1m 28s\n",
      "884:\tlearn: 1766.5314812\ttotal: 11m 12s\tremaining: 1m 27s\n",
      "885:\tlearn: 1766.4170424\ttotal: 11m 13s\tremaining: 1m 26s\n",
      "886:\tlearn: 1766.2519173\ttotal: 11m 14s\tremaining: 1m 25s\n",
      "887:\tlearn: 1766.1674017\ttotal: 11m 14s\tremaining: 1m 25s\n",
      "888:\tlearn: 1766.1076902\ttotal: 11m 15s\tremaining: 1m 24s\n",
      "889:\tlearn: 1765.9652690\ttotal: 11m 16s\tremaining: 1m 23s\n",
      "890:\tlearn: 1765.8399825\ttotal: 11m 16s\tremaining: 1m 22s\n",
      "891:\tlearn: 1765.6783244\ttotal: 11m 17s\tremaining: 1m 22s\n",
      "892:\tlearn: 1765.5923195\ttotal: 11m 18s\tremaining: 1m 21s\n",
      "893:\tlearn: 1765.5178901\ttotal: 11m 18s\tremaining: 1m 20s\n",
      "894:\tlearn: 1765.4658624\ttotal: 11m 19s\tremaining: 1m 19s\n",
      "895:\tlearn: 1765.2952160\ttotal: 11m 20s\tremaining: 1m 18s\n",
      "896:\tlearn: 1765.2143054\ttotal: 11m 21s\tremaining: 1m 18s\n",
      "897:\tlearn: 1765.0420062\ttotal: 11m 21s\tremaining: 1m 17s\n",
      "898:\tlearn: 1764.9741126\ttotal: 11m 22s\tremaining: 1m 16s\n",
      "899:\tlearn: 1764.7905367\ttotal: 11m 23s\tremaining: 1m 15s\n",
      "900:\tlearn: 1764.6249606\ttotal: 11m 24s\tremaining: 1m 15s\n",
      "901:\tlearn: 1764.4567080\ttotal: 11m 24s\tremaining: 1m 14s\n",
      "902:\tlearn: 1764.3779845\ttotal: 11m 25s\tremaining: 1m 13s\n",
      "903:\tlearn: 1764.2957015\ttotal: 11m 26s\tremaining: 1m 12s\n",
      "904:\tlearn: 1764.2503700\ttotal: 11m 27s\tremaining: 1m 12s\n",
      "905:\tlearn: 1764.1484476\ttotal: 11m 28s\tremaining: 1m 11s\n",
      "906:\tlearn: 1764.0190273\ttotal: 11m 28s\tremaining: 1m 10s\n",
      "907:\tlearn: 1763.9824224\ttotal: 11m 29s\tremaining: 1m 9s\n",
      "908:\tlearn: 1763.9156909\ttotal: 11m 30s\tremaining: 1m 9s\n",
      "909:\tlearn: 1763.8493218\ttotal: 11m 30s\tremaining: 1m 8s\n",
      "910:\tlearn: 1763.7744210\ttotal: 11m 31s\tremaining: 1m 7s\n",
      "911:\tlearn: 1763.6517364\ttotal: 11m 32s\tremaining: 1m 6s\n",
      "912:\tlearn: 1763.5732026\ttotal: 11m 33s\tremaining: 1m 6s\n",
      "913:\tlearn: 1763.4552478\ttotal: 11m 33s\tremaining: 1m 5s\n",
      "914:\tlearn: 1763.2967816\ttotal: 11m 34s\tremaining: 1m 4s\n",
      "915:\tlearn: 1763.2110831\ttotal: 11m 35s\tremaining: 1m 3s\n",
      "916:\tlearn: 1763.0711970\ttotal: 11m 36s\tremaining: 1m 3s\n",
      "917:\tlearn: 1762.8991493\ttotal: 11m 36s\tremaining: 1m 2s\n",
      "918:\tlearn: 1762.6799729\ttotal: 11m 37s\tremaining: 1m 1s\n",
      "919:\tlearn: 1762.6375990\ttotal: 11m 38s\tremaining: 1m\n",
      "920:\tlearn: 1762.4478641\ttotal: 11m 39s\tremaining: 1m\n",
      "921:\tlearn: 1762.3491105\ttotal: 11m 40s\tremaining: 59.3s\n",
      "922:\tlearn: 1762.2083489\ttotal: 11m 41s\tremaining: 58.5s\n",
      "923:\tlearn: 1762.0208019\ttotal: 11m 42s\tremaining: 57.7s\n",
      "924:\tlearn: 1761.9203016\ttotal: 11m 42s\tremaining: 57s\n",
      "925:\tlearn: 1761.7605050\ttotal: 11m 43s\tremaining: 56.2s\n",
      "926:\tlearn: 1761.6709062\ttotal: 11m 44s\tremaining: 55.5s\n",
      "927:\tlearn: 1761.5884750\ttotal: 11m 44s\tremaining: 54.7s\n",
      "928:\tlearn: 1761.5321486\ttotal: 11m 45s\tremaining: 53.9s\n",
      "929:\tlearn: 1761.4958128\ttotal: 11m 46s\tremaining: 53.2s\n",
      "930:\tlearn: 1761.3879611\ttotal: 11m 47s\tremaining: 52.4s\n",
      "931:\tlearn: 1761.2797925\ttotal: 11m 48s\tremaining: 51.7s\n",
      "932:\tlearn: 1761.1546261\ttotal: 11m 48s\tremaining: 50.9s\n",
      "933:\tlearn: 1761.0383166\ttotal: 11m 49s\tremaining: 50.1s\n",
      "934:\tlearn: 1760.9742551\ttotal: 11m 49s\tremaining: 49.3s\n",
      "935:\tlearn: 1760.8449217\ttotal: 11m 50s\tremaining: 48.6s\n",
      "936:\tlearn: 1760.7697638\ttotal: 11m 51s\tremaining: 47.8s\n",
      "937:\tlearn: 1760.6121779\ttotal: 11m 52s\tremaining: 47.1s\n",
      "938:\tlearn: 1760.4617078\ttotal: 11m 53s\tremaining: 46.3s\n",
      "939:\tlearn: 1760.2847433\ttotal: 11m 54s\tremaining: 45.6s\n",
      "940:\tlearn: 1760.1537019\ttotal: 11m 54s\tremaining: 44.8s\n",
      "941:\tlearn: 1760.0494130\ttotal: 11m 55s\tremaining: 44.1s\n",
      "942:\tlearn: 1759.9172253\ttotal: 11m 56s\tremaining: 43.3s\n",
      "943:\tlearn: 1759.8300184\ttotal: 11m 57s\tremaining: 42.6s\n",
      "944:\tlearn: 1759.6883064\ttotal: 11m 58s\tremaining: 41.8s\n",
      "945:\tlearn: 1759.6610011\ttotal: 11m 59s\tremaining: 41s\n",
      "946:\tlearn: 1759.5663201\ttotal: 11m 59s\tremaining: 40.3s\n",
      "947:\tlearn: 1759.4802639\ttotal: 12m\tremaining: 39.5s\n",
      "948:\tlearn: 1759.3457577\ttotal: 12m 1s\tremaining: 38.8s\n",
      "949:\tlearn: 1759.1984779\ttotal: 12m 2s\tremaining: 38s\n",
      "950:\tlearn: 1759.0280802\ttotal: 12m 3s\tremaining: 37.3s\n",
      "951:\tlearn: 1758.9205568\ttotal: 12m 3s\tremaining: 36.5s\n",
      "952:\tlearn: 1758.8328169\ttotal: 12m 4s\tremaining: 35.7s\n",
      "953:\tlearn: 1758.7073051\ttotal: 12m 5s\tremaining: 35s\n",
      "954:\tlearn: 1758.5359701\ttotal: 12m 6s\tremaining: 34.2s\n",
      "955:\tlearn: 1758.4376705\ttotal: 12m 7s\tremaining: 33.5s\n",
      "956:\tlearn: 1758.3309658\ttotal: 12m 7s\tremaining: 32.7s\n",
      "957:\tlearn: 1758.3049102\ttotal: 12m 8s\tremaining: 31.9s\n",
      "958:\tlearn: 1758.1772329\ttotal: 12m 9s\tremaining: 31.2s\n",
      "959:\tlearn: 1758.0530418\ttotal: 12m 10s\tremaining: 30.4s\n",
      "960:\tlearn: 1757.9930316\ttotal: 12m 10s\tremaining: 29.7s\n",
      "961:\tlearn: 1757.9049044\ttotal: 12m 11s\tremaining: 28.9s\n",
      "962:\tlearn: 1757.8366742\ttotal: 12m 12s\tremaining: 28.1s\n",
      "963:\tlearn: 1757.7715071\ttotal: 12m 12s\tremaining: 27.4s\n",
      "964:\tlearn: 1757.6622729\ttotal: 12m 13s\tremaining: 26.6s\n",
      "965:\tlearn: 1757.6101952\ttotal: 12m 14s\tremaining: 25.9s\n",
      "966:\tlearn: 1757.5361261\ttotal: 12m 15s\tremaining: 25.1s\n",
      "967:\tlearn: 1757.4382002\ttotal: 12m 15s\tremaining: 24.3s\n",
      "968:\tlearn: 1757.3469157\ttotal: 12m 16s\tremaining: 23.6s\n",
      "969:\tlearn: 1757.2906167\ttotal: 12m 17s\tremaining: 22.8s\n",
      "970:\tlearn: 1757.2280836\ttotal: 12m 17s\tremaining: 22s\n",
      "971:\tlearn: 1757.0679479\ttotal: 12m 18s\tremaining: 21.3s\n",
      "972:\tlearn: 1756.9258799\ttotal: 12m 19s\tremaining: 20.5s\n",
      "973:\tlearn: 1756.8553861\ttotal: 12m 19s\tremaining: 19.8s\n",
      "974:\tlearn: 1756.7784239\ttotal: 12m 20s\tremaining: 19s\n",
      "975:\tlearn: 1756.7363490\ttotal: 12m 21s\tremaining: 18.2s\n",
      "976:\tlearn: 1756.6142296\ttotal: 12m 22s\tremaining: 17.5s\n",
      "977:\tlearn: 1756.5408664\ttotal: 12m 22s\tremaining: 16.7s\n",
      "978:\tlearn: 1756.4435910\ttotal: 12m 23s\tremaining: 15.9s\n",
      "979:\tlearn: 1756.3307196\ttotal: 12m 24s\tremaining: 15.2s\n",
      "980:\tlearn: 1756.1841546\ttotal: 12m 24s\tremaining: 14.4s\n",
      "981:\tlearn: 1756.0858110\ttotal: 12m 25s\tremaining: 13.7s\n",
      "982:\tlearn: 1755.9676687\ttotal: 12m 26s\tremaining: 12.9s\n",
      "983:\tlearn: 1755.8817843\ttotal: 12m 27s\tremaining: 12.1s\n",
      "984:\tlearn: 1755.7726275\ttotal: 12m 28s\tremaining: 11.4s\n",
      "985:\tlearn: 1755.7463262\ttotal: 12m 28s\tremaining: 10.6s\n",
      "986:\tlearn: 1755.6990061\ttotal: 12m 29s\tremaining: 9.87s\n",
      "987:\tlearn: 1755.5847972\ttotal: 12m 30s\tremaining: 9.11s\n",
      "988:\tlearn: 1755.5164997\ttotal: 12m 30s\tremaining: 8.35s\n",
      "989:\tlearn: 1755.4229300\ttotal: 12m 31s\tremaining: 7.59s\n",
      "990:\tlearn: 1755.3946103\ttotal: 12m 32s\tremaining: 6.83s\n",
      "991:\tlearn: 1755.2812407\ttotal: 12m 33s\tremaining: 6.07s\n",
      "992:\tlearn: 1755.2385820\ttotal: 12m 33s\tremaining: 5.31s\n",
      "993:\tlearn: 1755.1308219\ttotal: 12m 34s\tremaining: 4.55s\n",
      "994:\tlearn: 1755.0739652\ttotal: 12m 35s\tremaining: 3.79s\n",
      "995:\tlearn: 1755.0049289\ttotal: 12m 35s\tremaining: 3.04s\n",
      "996:\tlearn: 1754.8750395\ttotal: 12m 36s\tremaining: 2.28s\n",
      "997:\tlearn: 1754.7337642\ttotal: 12m 37s\tremaining: 1.52s\n",
      "998:\tlearn: 1754.7098630\ttotal: 12m 38s\tremaining: 759ms\n",
      "999:\tlearn: 1754.6236614\ttotal: 12m 39s\tremaining: 0us\n",
      "1817.8497333668638\n",
      "CPU times: user 11min 42s, sys: 57.2 s, total: 12min 39s\n",
      "Wall time: 12min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = CatBoostRegressor()\n",
    "model.fit(X_train, y_train, cat_features=categorical)\n",
    "predict = model.predict(X_test)\n",
    "rmse = pd.np.sqrt(mean_squared_error(y_test, predict))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could likely try some more paramters with our CatBoost model, I would recommend sticking with LGBMRegressor using 200 leaves and a max depth of 20. \n",
    "\n",
    "If we want to further reduce RMSE, we can also explore further parameter tuning. \n",
    "\n",
    "To summarize findings, here is a simple table that shows the effectiveness of various approaches (based on using best parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Type of Model</strong></td>\n",
    "        <td><strong>Time to Complete</strong></td>\n",
    "        <td><strong>RMSE</strong></td>\n",
    "        <td><strong>Model Score</strong></td>\n",
    "        <td><strong>Total Time Tuning</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Linear Regression</td>\n",
    "        <td>A few seconds</td>\n",
    "        <td>3170.88</td>\n",
    "        <td>0.51</td>\n",
    "        <td>N/A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Random Forest</td>\n",
    "        <td>About 1 minute</td>\n",
    "        <td>1735.42</td>\n",
    "        <td>0.86</td>\n",
    "        <td>Over 2 Hours</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>LGBM</td>\n",
    "        <td>Less than 30 seconds</td>\n",
    "        <td>1694.40</td>\n",
    "        <td>0.866</td>\n",
    "        <td>About 21 minutes</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Catboost</td>\n",
    "        <td>11 Minutes, 42 Seconds</td>\n",
    "        <td>1813.83</td>\n",
    "        <td>N/A</td>\n",
    "        <td>11 Minutes, 42 Seconds</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
